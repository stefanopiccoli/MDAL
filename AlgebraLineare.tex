%PREAMBOLO
\documentclass[a4paper, 12pt]{report}
\usepackage[italian]{babel}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{amsbsy}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{multicol}
\renewcommand{\footnoterule}{
  \kern -3pt
  \hrule width \textwidth height 1pt
  \kern 2pt
}%ALLUNGA LINEA PIE DI PAGINA
\setcounter{tocdepth}{3}%AGGIUNGE SUBSUBSECTION ALL'INDICE
%INIZIO
\begin{document}
\title{
\textbf{Algebra Lineare}}
\author{Stefano Piccoli}
\date{\today}
\maketitle
\tableofcontents
	\chapter*{Introduzione}
	\addcontentsline{toc}{chapter}{Introduzione}
        \paragraph{}L'\textbf{Algebra Lineare} si occupa di trovare soluzioni ad equazioni e sistemi \textbf{lineari}.
            \begin{center} 
                $$\begin{cases}
                    E1: x+y=3\\
                    E2: x+2y=5
                \end{cases}$$            
                $E2-E1: \textbf{y=5-3=2}$\\
                Sostituzione: $\textbf{x=1}$
            
                $$\begin{cases}
                    E1: x+y=3\\
                    E2: 2x+2y=6
                \end{cases}$$
                $E2-E1:0=0$\\
                \textbf{Hanno le stesse soluzioni (infinità)}\\
            
                $$\begin{cases}
                    E1: x+y=3\\
                    E2: 2x+2y=5
                \end{cases}$$
                $E2-E1: 0=-1$\\
                \textbf{Nessuna soluzione comune} 
            \end{center}
        \paragraph{}Quindi abbiamo 1, $\infty$ o 0 soluzioni comuni. Così sarà in generale. 
        \section{Equazioni a 3 variabili}
            \paragraph{}Le soluzioni comuni di 3 equazioni lineari a 3 variabili corrispondono all'intersezione di 3 piani nello spazio tridimensionale.
            L'intersezione può essere di 3 tipi:
            \begin{itemize}
                \item Un punto (\textbf{unica soluzione})
                \item Una retta o un piano
                \item 0 (\textbf{$\infty$ soluzioni})
            \end{itemize}
        \section{Caso generale}
            \paragraph{}Un sistema di n equazioni lineari a m variabili.
            $$\begin{cases}
                a_{11}x_1+a_{12}x_2+\dots+a_{1m}x_m=b_1\\
                a_{12}x_1+a_{22}x_2+\dots+a_{2m}x_m=b_2\\
                \vdots\\
                a_{n1}x_1+a_{n2}x_2+\dots+a_{nm}x_m=b_m
            \end{cases}$$
            \begin{center}
                $a_{ij},b_i\in\Re$\\
                $n,m > 0$\\
            \end{center}
                \subsection{Sistema omogeneo}   
                    \paragraph{}Il sistema (E) è \textbf{omogeneo} se $b_1=b_2=$\dots$=b_n=0$ 
                    $$\begin{cases}
                        a_{11}x_1+a_{12}x_2+\dots+a_{1m}x_m=0\\
                        a_{12}x_1+a_{22}x_2+\dots+a_{2m}x_m=0\\
                        \vdots\\
                        a_{n1}x_1+a_{n2}x_2+\dots+a_{nm}x_m=0
                    \end{cases}$$
                \subsection{Sistema omogeneo associato}
                    \paragraph{}Un sistema omogeneo associato è un sistema dove la parte prima parte è uguale
                    ad un altro e i coefficienti dopo l'uguale sono \textbf{0}.
                \subsection{Soluzione di un sistema}
                    \paragraph{}\textbf{Soluzione di un sistema = soluzione di un caso particolare + soluzione dell'omogenea associata}.
                    \paragraph{Esempio} $2x+3y=5$, $n=1, m=2$
                        \subparagraph{Soluzione particolare}
                            \begin{align*}
                                &2x+3y=5\\
                                &x=y=1
                            \end{align*}
                        \subparagraph{Soluzione omogenea}
                            \begin{align*}
                                &2x+3y=0\\
                                &x=-\frac{3}{2}y
                            \end{align*}
                        \subparagraph{Soluzione generale}Definiamo s parametro nel ruolo di y.
                            \begin{align*}
                                &x=1+(-\frac{3}{2})s\\
                                &y=1+s
                            \end{align*}
                \subsection{Trovare soluzioni comuni}
                    \paragraph{}Per trovare soluzioni comuni di E è necessario semplificare.
                    Le 3 operazioni utili per semplificare sono:
                            \begin{enumerate}[label=\Alph*)]
                                \item Moltiplicare un'equazione $E_i$ per una costante. $\lambda \neq 0$. $E_i\Rightarrow\lambda E_i$ 
                                \item Moltiplicare un'equazione $E_i$ per $\lambda \neq 0$ e fare la somma con $E_j$.\\ $Ej\Rightarrow E_j+\lambda E_i$. 
                                \item Scambiare due equazioni.
                            \end{enumerate}
    \chapter{Matrici}
            \paragraph{}Per semplificare inseriamo i coefficienti delle equazioni in una \textbf{matrice $n\cdot m$}.
            $$
            \begin{bmatrix}
                a_{11} & a_{12} & \dots & a_{1m}\\
                a_{21} & a_{22} & \dots & a_{2m}\\
                \vdots\\
                a_{n1} & a_{n2} & \dots & a_{nm}
            \end{bmatrix}
            $$
            \subsection{Operazioni}
                Le operazioni che potevamo usare per semplificare il sistema possiamo utilizzarle anche sulle matrici:
                    \begin{enumerate}[label=\Alph*)]
                        \item Moltiplicare una riga per $\lambda \neq 0$. $R_i \Rightarrow \lambda \cdot R_i$.
                        \item Sostituire la riga $R_j$ con una somma. $R_j \Rightarrow R_j + \lambda \cdot R_i$.
                        \item Scambiare due righe.
                    \end{enumerate}
            \section{Matrice a scalini}
                \paragraph{}Una matrice $n \cdot m$  è detta a \textbf{a scalini} se:
                    \begin{enumerate}
                        \item Le righe sono \textbf{in fondo}.
                        \item Il primo elemento di ogni riga, se esiste, è \textbf{a destra} del primo elemento $\neq 0$ della riga precedente. Un tale elemento è detti \textbf{Pivot}.
                    \end{enumerate}
                        \begin{center}
                        $
                        \begin{bmatrix}
                            1 & 1 & 1\\
                            1 & 0 & 0\\
                            0 & 0 & 1
                        \end{bmatrix}
                        NO
                        $
                        $
                        \begin{bmatrix}
                            1 & 1 & 1\\
                            0 & 1 & 1\\
                            0 & 0 & 1
                        \end{bmatrix}
                        $
                        SI
                        $
                        \begin{bmatrix}
                            0 & 1 & 1\\
                            1 & 1 & 0\\
                            0 & 0 & 1
                        \end{bmatrix}
                        $
                        NO
                    \end{center}
            \section{Algoritmo di Gauss}
                \begin{enumerate}
                    \item Se la matrice è gia in forma a \textbf{scalini} si termina. \textbf{END}.
                    \item Si cerca il primo elemento $\neq 0$ della prima colonna $\neq 0$.
                    \item Scambiando le righe possiamo supporre che questo elemento è il \textbf{pivot} della prima riga. Lo segniamo con \textit{p}. 
                    \item Se siamo in forma a scalini si \textbf{termina}. \textbf{END}.
                    \item Si annullano tutti gli elementi della colonna di \textit{p} con operazioni di tipo $R_j \Rightarrow R_j + \lambda \cdot R_i$.
                    \item Se siamo in forma a scalini si \textbf{termina}. \textbf{END}.
                    \item Si ricomincia con la matrice ottenuta \textbf{escludendo} la prima riga.
                \end{enumerate}
                \subparagraph{Esempio}
                $$
                \begin{bmatrix}
                    \textcolor{orange}{1} & -1 & 0 & 3\\
                    3 & -1 & 1 & 10\\
                    1 & 5 & 2 & 1
                \end{bmatrix}
                $$
                \paragraph{}Il \textbf{pivot} della prima riga è 1, ora devo annullare tutti gli elementi della colonna del pivot.
                $$
                    \xrightarrow[]{R_2-3R_1}                    
                    \begin{bmatrix}
                        \textcolor{orange}{1} & -1 & 0 & 3\\
                        \textcolor{blue}{0} & 2 & 1 & 1\\
                        1 & 5 & 2 & 1
                    \end{bmatrix}
                    \xrightarrow[]{R_3-R_1}                    
                    \begin{bmatrix}
                        \textcolor{orange}{1} & -1 & 0 & 3\\
                        \textcolor{blue}{0} & 2 & 1 & 1\\
                        \textcolor{blue}{0} & 6 & 2 & -2
                    \end{bmatrix}
                $$
                La prima riga è \textbf{completata}, si ripete l'algoritmo escludendola.
                $$
                    \begin{bmatrix}
                        \textcolor{orange}{1} & -1 & 0 & 3\\
                        \hline
                        \textcolor{blue}{0} & \textcolor{orange}{2} & 1 & 1\\
                        \textcolor{blue}{0} & 6 & 2 & -2
                    \end{bmatrix}
                $$
                Nella seconda riga il \textbf{pivot} è 2, si procede annullando le colonne sotto il pivot.
                $$
                    \xrightarrow[]{R_3-R_1}
                    \begin{bmatrix}
                        \textcolor{orange}{1} & -1 & 0 & 3\\
                        \hline
                        \textcolor{blue}{0} & \textcolor{orange}{2} & 1 & 1\\
                        \textcolor{blue}{0} & 6 & 2 & -2
                    \end{bmatrix}
                    \xrightarrow[]{R_3-3R_2}
                    \begin{bmatrix}
                        \textcolor{orange}{1} & -1 & 0 & 3\\
                        \hline
                        \textcolor{blue}{0} & \textcolor{orange}{2} & 1 & 1\\
                        \textcolor{blue}{0} & \textcolor{blue}{0} & -1 & 5
                    \end{bmatrix}
                $$
                La seconda riga è \textbf{completata}, si ripete l'algoritmo escludendola.
                $$
                \begin{bmatrix}
                    \textcolor{orange}{1} & -1 & 0 & 3\\
                    \textcolor{blue}{0} & \textcolor{orange}{2} & 1 & 1\\
                    \hline
                    \textcolor{blue}{0} & \textcolor{blue}{0} & \textcolor{orange}{-1} & 5
                \end{bmatrix}
                $$
                L'algoritmo termina poiché -1 è un \textbf{pivot} e non ci sono colonne da annullare.
                \subparagraph{Conclusioni}La matrice ritrasformata in sistema di equazioni è la seguente:
                $$
                \begin{cases}
                    x_1-x_2+3x_4=0\\
                    2x_2+x_3+x_4=0\\
                    -x_3-5x_4=0
                \end{cases}
                $$
                La \textbf{colonna di $x_4$} è senza \textbf{pivot} quindi \textbf{$x_4$} è detta \textbf{variabile libera}, e può assumere qualsiasi
                valore nel sistema.
                Sostituiamo la \textbf{variabile libera} $x_4$ con il parametro \textit{t}.
                $$
                \begin{cases}
                    x_1-x_2+3t=0\\
                    2x_2+x_3+t=0\\
                    -x_3-5t=0
                \end{cases}
                \begin{cases}
                    x_1-x_2+3t=0\\
                    2x_2+x_3+t=0\\
                    x_3=-5t
                \end{cases}
                \begin{cases}
                    x_1-x_2+3t=0\\
                    2x_2-5t+t=0\\
                    x_3=-5t
                \end{cases}
                $$
                $$
                \begin{cases}
                    x_1-x_2+3t=0\\
                    x_2=2t\\
                    x_3=-5t
                \end{cases}
                \begin{cases}
                    x_1-2t+3t=0\\
                    x_2=2t\\
                    x_3=-5t
                \end{cases}
                \begin{cases}
                    x_1=-t\\
                    x_2=2t\\
                    x_3=-5t
                \end{cases}
                $$ 
            L'equazione ha \textbf{infinite soluzioni} che possono essere parametrizzate in \textit{t}.
            \subsection{Casi possibili}
            \paragraph{}Se nella forma a scalini:
                \begin{enumerate}
                    \item \textbf{Ogni colonna} "non aggiunta" \textbf{ha un pivot} $\Leftrightarrow \exists$ \textbf{unica soluzione}
                    \item C'è un \textbf{pivot nell'ultima colonna} $\Leftrightarrow \nexists$ \textbf{soluzione}
                    \item C'è una \textbf{colonna "non aggiunta" senza pivot} e l'ultima colonna non ne ha $\Leftrightarrow \exists \; \infty$ \textbf{soluzioni} 
                \end{enumerate}
        \clearpage
        \section{Matrice ridotta a scalini}
            \paragraph{}Una matrice è in forma \textbf{ridotta} a scalini se:
                \begin{itemize}
                    \item È in forma \textbf{a scalini}
                    \item Ogni \textbf{pivot} è = 1
                    \item Ogni \textbf{pivot} è l'unico elemento $\neq 0$ nella sua colonna
                \end{itemize}
                \subparagraph{Esempi}
                $$
                \begin{bmatrix}
                    \textcolor{blue}{1} & 2 & 0 & 0\\
                    0 & 0 & \textcolor{blue}1 & 0\\
                    0 & 0 & 0 & \textcolor{blue}{1}
                \end{bmatrix}
                \text{SI }
                \begin{bmatrix}
                    \textcolor{blue}{1} & 2 & \textcolor{orange}{3} & \textcolor{orange}{4}\\
                    0 & 0 & \textcolor{blue}{1} & \textcolor{orange}{2}\\
                    0 & 0 & 0 & \textcolor{blue}{1}
                \end{bmatrix}
                \text{NO (A scalini ma non ridotta)}
                $$
        \section{Algoritmo di Gauss-Jordan}
            \paragraph{}L'algoritmo produce una matrice in forma \textbf{ridotta} a scalini attraverso operazioni
            del tipo A, B, C.
                \begin{enumerate}
                    \item Con l'\textbf{algoritmo di Gauss} si riduce a scalini la matrice.
                    \item Nelle colonne dei pivot gli elementi della colonna superiore e a sinistra nella riga sono già = 0.
                    \textbf{Annullare} gli elementi sopra il pivot nella colonna con \textbf{operazioni del tipo B} ($R_j \Rightarrow R_j + \lambda \cdot R_i$).
                    \item In ogni riga si \textbf{cerca il pivot} (se esiste). Se il pivot $\lambda \neq 1$, si moltiplica la riga per $\frac{1}{\lambda}$.
                \end{enumerate}
                \subparagraph{Esempio}Partiamo da una matrice già ridotta a scalini dall'algoritmo di Gauss.
                    $$
                    \begin{bmatrix}
                        2 & 1 & -1 & \vline & -1\\
                        3 & 2 & -1 & \vline & 0\\
                        4 & -3 & 1 & \vline & -1\\
                        5 & -2 & 2 & \vline & 2
                    \end{bmatrix}
                    \xrightarrow[]{\text{Algoritmo di Gauss}}
                    \begin{bmatrix}
                        \textcolor{blue}{2} & 1 & -1 & \vline & -1\\
                        0 & \textcolor{blue}{1} & 1 & \vline & 3\\
                        0 & 0 & \textcolor{blue}{1} & \vline & 2\\
                        0 & 0 & 0 & \vline & 0
                    \end{bmatrix}
                    $$ 
                    Ora applichiamo l'\textbf{algoritmo di Gauss-Jordan} alla matrice a scalini per trasformarla in matrice \textbf{ridotta} a scalini.
                    $$
                    \begin{bmatrix}
                        \textcolor{blue}{2} & 1 & -1 & \vline & -1\\
                        0 & \textcolor{blue}{1} & 1 & \vline & 3\\
                        0 & 0 & \textcolor{blue}{1} & \vline & 2\\
                        0 & 0 & 0 & \vline & 0
                    \end{bmatrix}
                    $$
                    Si \textbf{azzerano} gli elementi nelle colonne dei pivot che sono $\neq$ 0.
                    $$
                    \begin{bmatrix}
                        2 & \textcolor{orange}{1} & \textcolor{orange}{-1} & \vline & -1\\
                        0 & 1 & \textcolor{orange}{1} & \vline & 3\\
                        0 & 0 & 1 & \vline & 2\\
                        0 & 0 & 0 & \vline & 0
                    \end{bmatrix}
                    \xrightarrow[]{R_1-R_2}
                    \begin{bmatrix}
                        2 & \textcolor{blue}{0} & \textcolor{orange}{-2} & \vline & -4\\
                        0 & 1 & \textcolor{orange}{1} & \vline & 3\\
                        0 & 0 & 1 & \vline & 2\\
                        0 & 0 & 0 & \vline & 0
                    \end{bmatrix}
                    \xrightarrow[R_2-R_3]{R_1+2R_3}
                    \begin{bmatrix}
                        2 & \textcolor{blue}{0} & \textcolor{blue}{0} & \vline & 0\\
                        0 & 1 & \textcolor{blue}{0} & \vline & 1\\
                        0 & 0 & 1 & \vline & 2\\
                        0 & 0 & 0 & \vline & 0
                    \end{bmatrix}
                    $$
                    Ora nelle colonne dei pivot \textbf{tutti gli elementi sono = 0} eccetto il pivot.
                    Si individuano i \textbf{pivot} $\neq 1$ e si procede con la loro \textbf{trasformazione a 1}.
                    Si moltiplicano le righe con i \textbf{pivot} $\neq$ 1 per il loro \textbf{reciproco}.
                    $$
                    \begin{bmatrix}
                        \textcolor{orange}{2} & 0 & 0 & \vline & 0\\
                        0 & 1 & 0 & \vline & 1\\
                        0 & 0 & 1 & \vline & 2\\
                        0 & 0 & 0 & \vline & 0
                    \end{bmatrix}
                    \xrightarrow[]{R_1\rightarrow \frac{1}{2}R_1}
                    \begin{bmatrix}
                        \textcolor{blue}{1} & 0 & 0 & \vline & 0\\
                        0 & 1 & 0 & \vline & 1\\
                        0 & 0 & 1 & \vline & 2\\
                        0 & 0 & 0 & \vline & 0
                    \end{bmatrix}
                    $$
                \subparagraph{Conclusioni}
                $$
                \begin{cases}
                x_1=0\\
                x_2=1\\
                x_3=2  
                \end{cases}
                $$
    \chapter{Spazi vettoriali}
        \paragraph{}Si parla di \textbf{spazi vettoriali} quando definiamo punti e vettori
        nel piano $\mathbb{R}^2$. Un \textbf{punto} di $\mathbb{R}^2$ si può descrivere con \textbf{due coordinate}
        $(x_1,x_2)$, ma anche con un \textbf{vettore} (una freccia) dall'\textbf{origine} $(0,0)$ a $(x_1,x_2)$ 
            \subsection{Somma}
            \paragraph{}Si può fare la \textbf{somma} di due vettori:
                \begin{itemize}
                    \item Sulle \textbf{coordinate}: $(x_1,x_2)+(x'_1+x'_2):=(x_1+x'_1,x_2+x'_2)$
                    \item \textbf{Geometricamente}: \textbf{Legge del parallelogramma} dove la \textbf{diagonale del parallelogramma} è la somma dei due vettori.
                \end{itemize}
            \subsection{Moltiplicazione}
            \paragraph{}Un vettore può essere moltiplicato con uno scalare $\lambda \in \mathbb{R}$.
            \begin{itemize}
                \item Sulle \textbf{coordinate}: $\lambda (x_1,x_2):=(\lambda x_1,\lambda x_2)$
                \item \textbf{Geometricamente}: La \textbf{lunghezza} è moltiplicata da $\lambda$ ma l'angolo non cambia.
            \end{itemize}
        \section{Spazi vettoriali di dimensione n}
            \paragraph{}Si definisce $\mathbb{R}^n:= \left \{
            \begin{bmatrix}
            x_1\\
            x_2\\
            \vdots\\
            x_n    
            \end{bmatrix}
            : x_i \in \mathbb{R}
            \right \}
            $
            uno \textbf{spazio n-dim standard} o spazio dei vettori colonna.\\
            Un spazio vettoriale di dimensione \textbf{2} corrisponde ad un \textbf{piano}, di dimensione \textbf{3} ad uno
            spazio \textbf{euclideiano}.
            \paragraph{Definizione} Uno \textbf{spazio vettoriale} su $\mathbb{R}$ è un insieme V che ammette due tipi di operazioni:
                \begin{itemize}
                    \item \textbf{Somma}: $v_1,v_2 \in V \rightarrow v_1+v_2 \in V$.
                    \item \textbf{Prodotto} con $\lambda \in \mathbb{R}$ : $v \in V \rightarrow \lambda \cdot v \in V$. 
                \end{itemize}
            Le operazioni devono soddisfare:
                \begin{multicols}{2}
                \begin{enumerate}              
                    \item $(v_1+v_2)+v_3 = v_1+(v_2+v_3)$
                    \item $v_1+v_2 = v_2+v_1$
                    \item \footnote{$\exists !$= Esiste un unico}$\exists ! 0 \in V: 0+v=v+0=v \; \forall v$
                    \item $\forall v \; \exists ! -v \in V: v+(-v)=(-v)+v=0$
                    \columnbreak
                    \item $(\lambda_1+\lambda_2)\cdot v = \lambda_1 \cdot v +\lambda_2 \cdot v$
                    \item $\lambda \cdot(v_1+v_2)=\lambda \cdot v_1+\lambda \cdot v_2$
                    \item $(\lambda_1 \cdot \lambda_2)\cdot v = \lambda_1 \cdot(\lambda_2 \cdot v)$
                    \item $1 \cdot v = v$
                \end{enumerate}
                \end{multicols}
            \subsection{Somma}
                $$
                \begin{bmatrix}
                    x_1\\
                    x_2\\
                    \vdots\\
                    x_n    
                \end{bmatrix}
                +
                \begin{bmatrix}
                    x'_1\\
                    x'_2\\
                    \vdots\\
                    x'_n    
                \end{bmatrix}
                :=
                \begin{bmatrix}
                    x_1+x'_1\\
                    x_2+x'_2\\
                    \vdots\\
                    x_n+x'_n    
                \end{bmatrix}
                $$
            \subsection{Moltiplicazione}
                $$
                \lambda \cdot
                \begin{bmatrix}
                    x_1\\
                    x_2\\
                    \vdots\\
                    x_n    
                \end{bmatrix}
                := 
                \begin{bmatrix}
                    \lambda \cdot x_1\\
                    \lambda \cdot x_2\\
                    \vdots\\
                    \lambda \cdot x_n    
                \end{bmatrix}
                \lambda \in \mathbb{R}.
                $$
            \clearpage
            \section{Sottospazi vettoriali}
                \paragraph{}Sia V uno spazio vettoriale. Un \textbf{sottospazio} $W \subset V$ è
                un sottoinsieme tale che
                    \begin{itemize}
                        \item Dati due vettori nel sottospazio, la loro somma sarà nel sottospazio. $$v_1,v_2 \in W \Rightarrow v_1+v_2 \in W$$
                        \item Dato un vettore nel sottospazio, il prodotto con un qualsiasi scalare è contenuto nel sottospazio. $$v \in W \Rightarrow \lambda v \in W \; \forall \lambda$$
                    \end{itemize} 
                Un sottospazio $\boldsymbol{W \subset V}$ \textbf{è uno spazio vettoriale}.
                \subparagraph{Esempio}
                    \begin{enumerate}
                        \item $ \left \{
                                \begin{bmatrix}
                                t_1\\
                                t_2
                                \end{bmatrix}
                                \in \mathbb{R}^2 : t_1+t_2=0
                                \right \}\subset \mathbb{R}^2
                                $ 
                                è un sottospazio.\\[2pt]
                                In generale
                                $$ \left \{
                                \begin{bmatrix}
                                t_1\\
                                t_2\\
                                \vdots\\
                                t_m
                                \end{bmatrix}
                                \in \mathbb{R}^m : 
                                \begin{cases}
                                    a_{11}t_1+a_{12}t_2+\dots+a_{1m}t_m=0\\
                                    a_{21}t_1+a_{22}t_2+\dots+a_{2m}t_m=0\\
                                    \vdots\\
                                    a_{n1}t_1+a_{n2}t_2+\dots+a_{nm}t_m=0
                                \end{cases}
                                \right \}\subset \mathbb{R}^m
                                $$
                                è \textbf{sottospazio}.\\
                                Quindi le \textbf{soluzioni} di un \textbf{sistema di equazioni lineari omogenei} a n
                                variabili definiscono un \textbf{sottospazio} di $\mathbb{R}^m$.\\
                                \textbf{Non definiscono un sottospazio} di $\mathbb{R}^m$ le soluzioni di equazioni lineari \textbf{non omogenee} (coefficiente $\neq 0$).
                    \end{enumerate}
            \clearpage
            \section{Combinazioni lineari}
                \paragraph{}Sia V uno spazio vettoriale, $v_1,v_2,\dots,v_m \in V$.
                Una \textbf{combinazione lineare} di $v_1,\dots,v_m$ è una somma $\boldsymbol{\lambda_1 v_1+\lambda_2 v_2+\dots+\lambda_m v_m \in V}$, dove $\lambda_1,\lambda_2,\dots,\lambda_m \in \mathbb{R}$.\\
                La combinazione lineare è detta \textbf{banale} se $\lambda_1=\dots=\lambda_m=0$.
                \subparagraph{Esempio}
                    $$
                    V=\mathbb{R}^2,\;v_1=
                    \begin{bmatrix}
                        1\\
                        1
                    \end{bmatrix}
                    ,\;v_2=
                    \begin{bmatrix}
                        2\\
                        2
                    \end{bmatrix}
                    $$
                    \subparagraph{}Allora $-2v_1+1v_2=0$ è \textbf{combinazione lineare non banale}.
        \section{Span}
            \paragraph{}Siano $v_1,\dots,v_m \in V \;m$ vettori. Il \textbf{sottospazio generato} da $v_1,\dots,v_m$ è:
            $$
                Span(v_1,v_2,...,v_m):= \left \{\lambda_1v_1+\lambda_2v_2+\dots+\lambda_mv_m\;:\;\lambda_1,\dots,\lambda_m \in \mathbb{R}\right\}
            $$   
            Quindi \textit{Span} è l'insieme di \textbf{tutte le combinazioni lineari}.\\
            $Span(v_1,v_2,\dots,v_m) \subset V$ è un \textbf{sottospazio}.     
                \subparagraph{Esempi}
                \begin{enumerate}
                    \item 
                        $$
                        \mathbb{R}^2 = Span \left\{
                            \begin{bmatrix}
                                0\\
                                1
                            \end{bmatrix}
                            ,
                            \begin{bmatrix}
                                1\\
                                0
                            \end{bmatrix}
                            \right\}
                        $$ 
                        $
                        Span \left\{
                        \begin{bmatrix}
                            0\\
                            1
                        \end{bmatrix}
                        \right\},Span
                        \left\{
                        \begin{bmatrix}
                            1\\
                            0
                        \end{bmatrix}
                        \right\} \subset \mathbb{R}^2
                        $    
                        sono due rette, rispettivamente dell'asse x e y.
                    \item 
                        $$
                        W := \left\{
                            \begin{bmatrix}
                                t_1\\
                                t_2\\
                                t_3
                            \end{bmatrix}
                            \in \mathbb{R}^3 : t_1=0 \right\}
                        $$
                        Allora
                        $
                        W=Span \left\{
                            \begin{bmatrix}
                                0\\
                                1\\
                                0
                            \end{bmatrix}
                            ,
                            \begin{bmatrix}
                                0\\
                                0\\
                                1
                            \end{bmatrix}
                            \right\}=Span \left\{
                                \begin{bmatrix}
                                    0\\
                                    1\\
                                    1
                                \end{bmatrix}
                                ,
                                \begin{bmatrix}
                                    0\\
                                    0\\
                                    -1
                                \end{bmatrix}
                                \right\}
                        $.\\
                        Quindi un \textbf{sottospazio} può essere lo \textbf{span di vettori diversi}.
                \end{enumerate}
                    \subsection{Esercizi}
                \paragraph{Verificare che $\boldsymbol{Span(v_1,v_2,v_3)=Span(v_1,v_2,v_3,v_4)=\mathbb{R}^3}$}
                    $$
                    v_1=
                    \begin{bmatrix}
                        1\\
                        2\\
                        3\\
                    \end{bmatrix},
                    v_2=
                    \begin{bmatrix}
                        1\\
                        0\\
                        1
                    \end{bmatrix},
                    v_3=
                    \begin{bmatrix}
                        0\\
                        0\\
                        1
                    \end{bmatrix},
                    v_4=
                    \begin{bmatrix}
                        2\\
                        2\\
                        4
                    \end{bmatrix}
                    $$
                    Se 
                    $
                    v=
                    \begin{bmatrix}
                        b_1\\
                        b_2\\
                        b_3\\
                    \end{bmatrix}
                    \in \mathbb{R}^3
                    $
                    applicando l'\textbf{Algoritmo di Gauss} si ottiene:
                    $$
                    \begin{bmatrix}
                        1 & 1 & 0 & \vline & b_1\\
                        2 & 0 & 0 & \vline & b_2\\
                        3 & 1 & 1 & \vline & b_3
                    \end{bmatrix}
                    \xrightarrow[R_3-3R_1]{R_2-2R_1}
                    \begin{bmatrix}
                        \textcolor{blue}{1} & 1 & 0 & \vline & b_1\\
                        0 & -2 & 0 & \vline & b_2-2b_1\\
                        0 & -2 & 1 & \vline & b_3-3b_1
                    \end{bmatrix}
                    $$
                    $$
                    \xrightarrow{R3-R2}
                    \begin{bmatrix}
                        \textcolor{blue}{1} & 1 & 0 & \vline & b_1\\
                        0 & \textcolor{blue}{-2} & 0 & \vline & b_2-2b_1\\
                        0 & 0 & \textcolor{blue}{1} & \vline & b_3-b_1-b_2
                    \end{bmatrix}
                    $$
                    3 \textbf{pivots} nelle 3 colonne a sinistra (non ci interessa a destra) quindi
                    $$
                    \begin{cases}
                        x_1+x_2=b_1\\
                        2x_1=b_2\\
                        3x_1+x_2+x_3=b_3
                    \end{cases}
                    $$
                    ammette un' \textbf{unica soluzione} $\lambda_1,\lambda_2,\lambda_3$:
                    $$
                    \lambda_1
                    \begin{bmatrix}
                        1\\
                        2\\
                        3
                    \end{bmatrix}
                    +\lambda_2
                    \begin{bmatrix}
                        1\\
                        0\\
                        1
                    \end{bmatrix}
                    +\lambda_3
                    \begin{bmatrix}
                        0\\
                        0\\
                        1
                    \end{bmatrix}
                    =
                    \begin{bmatrix}
                        b_1\\
                        b_2\\
                        b_3
                    \end{bmatrix}
                    \text{.}
                    $$
                    Il \textbf{vettore generale} v è contenuto in $Span(v_1,v_2,v_3)$.
                    \paragraph{In generale} Se $v_1,v_2,\dots,v_n \in V$ sono vettori tali che $v_n$ è \textbf{combinazione lineare}
                    di $v_1,v_2,\dots,v_{n-1} \Rightarrow Span(v_1,v_1,\dots,v_n)=Span(v_1,v_1,\dots,v_{n-1})$.
                    \paragraph{Trovare sistema di equazioni lineari omogenee tale che il sottospazio di $\boldsymbol{\mathbb{R}^n \text{ associato sia }Span(v_1,\dots,v_m)}$}
                    \begin{enumerate}
                        \item Si sceglie una base di $Span(v_1,v_2,\dots,v_m)$. Possiamo supporre la base $(v_1,\dots,v_r) \text{ con }r\leq m $.
                        \item Siano 
                            $
                            v_1=
                            \begin{bmatrix}
                                a_{11}\\
                                \vdots\\
                                a_{n1}
                            \end{bmatrix}
                            ,...,v_r=
                            \begin{bmatrix}
                                a_{1r}\\
                                \vdots\\
                                a_{nr}
                            \end{bmatrix}
                            \Rightarrow
                            A=
                            \begin{bmatrix}
                                a_{11} & \dots & a_{1r}\\
                                \vdots & \vdots & \vdots\\
                                a_{n1} & \dots & a_{nr}
                            \end{bmatrix}
                            $\\
                            $v_1,\dots,v_n$ \textbf{linearmente indipendenti} $\Leftrightarrow$ nella forma a scalini di A c'è un \textbf{pivot in ogni colonna}.
                        \item Sia
                            $
                            v=
                            \begin{bmatrix}
                                b_1\\
                                \vdots\\
                                b_n
                            \end{bmatrix}
                            $ qualsiasi.\\
                            $v \in Span(v_1,...,v_r) \Leftrightarrow \boldsymbol{v},v_1,\dots,v_r$ sono \textbf{linearmente dipendenti} $\Leftrightarrow$\\
                            nella forma a scalini della matrice 
                            $
                            \begin{bmatrix}
                                a_{11} & \dots & a_{1r} & b_1\\
                                \vdots & \vdots & \vdots & \vdots\\
                                a_{n1} & \dots & a_{nr} & b_n
                            \end{bmatrix}
                            $
                            ci sono sempre \textbf{r pivot} nelle prime \textbf{r colonne} ovvero \textbf{l'ultima colonna non contiene pivots}.\\
                            \textbf{Questo dà equazioni lineari per} $\boldsymbol{b_1,\dots,b_n}$.
                            \subparagraph{Esempio}\mbox{}\\
                            $
                            v_1=
                            \begin{bmatrix}
                                1\\
                                1\\
                                1
                            \end{bmatrix}
                            , v_2=
                            \begin{bmatrix}
                                1\\
                                3\\
                                1
                            \end{bmatrix}
                            $sono vettori \textbf{linearmente indipendenti} perché non sono multipli tra loro.
                            $$
                            \begin{bmatrix}
                                1 & 1 & b_1\\
                                1 & 3 & b_2\\
                                1 & 1 & b_3
                            \end{bmatrix}
                            \xrightarrow[R_3-R_1]{R_2-R_1}
                            \begin{bmatrix}
                                \textcolor{blue}{1} & 1 & b_1\\
                                0 & \textcolor{blue}{2} & b_2-b_1\\
                                0 & 0 & \textcolor{orange}{b_3-b_1}
                            \end{bmatrix}
                            $$
                            Il \textbf{pivot} da controllare è nell'ultima colonna quindi se $b_3-b_1=0 \Leftrightarrow$ \textbf{non} è un pivot della terza colonna.\\
                            Quindi $Span(v_1,v_2)=\{\text{soluzioni di }x_3-x_1=0\}$
                    \end{enumerate}
        \section{Dipendenza lineare}
            \paragraph{}I vettori $v_1,v_2,\dots,v_m \in V$ sono \textbf{linearmente indipendenti} se
            $$\lambda v_1+\lambda_2 v_2+\dots+\lambda_m V_m = 0$$ vale \textbf{solo} per $\lambda_1=\dots=\lambda_m=0$.
            Altrimenti sono \textbf{linearmente dipendenti}.
            \paragraph{Geometricamente} Vettori linearmente dipendenti hanno la \textbf{stessa retta}.
            \paragraph{Proposizione} $v_1,v_2,\dots,v_m$ sono \textbf{linearmente dipendenti} $\Leftrightarrow \exists i : v_i$ è combinazione lineare dei $v_j\; \forall j\neq i$.
            \paragraph{Verificare se m vettori sono linearmente indipendenti}
                $$
                v_1=
                \begin{bmatrix}
                    a_{11}\\
                    a_{21}\\
                    \vdots\\
                    a_{n1}       
                \end{bmatrix}
                ,v_2=
                \begin{bmatrix}
                    a_{12}\\
                    a_{22}\\
                    \vdots\\
                    a_{n2} 
                \end{bmatrix}
                ,\dots,\;
                v_m=
                \begin{bmatrix}
                    a_{1m}\\
                    a_{2m}\\
                    \vdots\\
                    a_{nm} 
                \end{bmatrix}
                $$
                L'equazione $\lambda_1v_1+\lambda_2v_2+\dots+\lambda_mv_m=0$ \textbf{vale se e solo se} $(\lambda_1,\dots,\lambda_m)$ \textbf{è soluzione del sistema}
                $$
                    \begin{cases}
                        a_{11}x_1+a_{12}x_2+\dots+a_{1m}x_m = 0\\
                        a_{21}x_1+a_{22}x_2+\dots+a_{1m}x_m = 0\\
                        \vdots\\
                        a_{n1}x_1+a_{n2}x_2+\dots+a_{nm}x_m = 0
                    \end{cases}
                $$               
                dove \textbf{x sostituisce} $\boldsymbol{\lambda}$ e lo 0 dell'equazione corrisponde al vettore
                $
                \begin{bmatrix}
                    0\\
                    \vdots\\
                    0
                \end{bmatrix}
                $.\\
                Quindi $v_1,\dots,v_m$ sono \textbf{linearmente indipendenti} $\Leftrightarrow$ il sistema ammette \textbf{solo la soluzione banale}, cioè $x=(0,\dots,0)$.
                \paragraph{Esempio} Verificare che i seguenti vettori di $\mathbb{R}^3$ siano \textbf{linearmente indipendenti}.
                    $$
                    v_1=
                    \begin{bmatrix}
                        1\\
                        2\\
                        3\\
                    \end{bmatrix},
                    v_2=
                    \begin{bmatrix}
                        1\\
                        0\\
                        1
                    \end{bmatrix},
                    v_3=
                    \begin{bmatrix}
                        0\\
                        0\\
                        1
                    \end{bmatrix},
                    v_4=
                    \begin{bmatrix}
                        2\\
                        2\\
                        4
                    \end{bmatrix}
                    $$
                Dobbiamo cercare le soluzioni del sistema \textbf{lineare omogeneo} con la \textbf{matrice dei coefficienti associata}.
                    $$
                    \begin{bmatrix}
                        1 & 1 & 0 & 2\\
                        2 & 0 & 0 & 2\\
                        3 & 1 & 1 & 4
                    \end{bmatrix}
                    $$
                \textbf{Algoritmo di Gauss:}
                    $$
                    \xrightarrow[R_3-3R_1]{R_2-2R_1}
                    \begin{bmatrix}
                        \textcolor{blue}{1} & 1 & 0 & 2\\
                        0 & -2 & 0 & -2\\
                        0 & -2 & 1 & -2
                    \end{bmatrix}
                    \xrightarrow{R_3-R_2}
                    \begin{bmatrix}
                        \textcolor{blue}{1} & 1 & 0 & 2\\
                        0 & \textcolor{blue}{-2} & 0 & -2\\
                        0 & 0 & \textcolor{blue}{1} & 0
                    \end{bmatrix}
                    $$
                Ci sono 3 \textbf{pivots} e una \textbf{variabile libera} $\Rightarrow \boldsymbol{\infty}$ \textbf{soluzioni}.\\
                Il sistema ammette \textbf{soluzioni non banali} $\Rightarrow$ i vettori sono \textbf{linearmente dipendenti}.
        \section{Basi}
            \paragraph{}Un sistema $v_1,\dots,v_n$ di vettori è una \textbf{base} di V se:
            \begin{itemize}
                \item i vettori $v_1,\dots,v_n$ sono \textbf{linearmente indipendenti}
                \item $\boldsymbol{Span(v_1,\dots,v_n) = V}$
            \end{itemize}
        %-33
            \paragraph{Esempio} Base standard di $\mathbb{R}^n$:
                $$
                e_1=
                \begin{bmatrix}
                    1\\
                    0\\
                    \vdots\\
                    0
                \end{bmatrix}
                ,e_2=
                \begin{bmatrix}
                    0\\
                    1\\
                    \vdots\\
                    0
                \end{bmatrix}
                ,\dots,e_n=
                \begin{bmatrix}
                    0\\
                    0\\
                    \vdots\\
                    1
                \end{bmatrix}
                $$
                Si osserva $
                    \begin{bmatrix}
                        \lambda_1\\
                        \lambda_2\\
                        \vdots\\
                        \lambda_n\\
                    \end{bmatrix}
                = \lambda_1 e_1+\lambda_2 e_2+\dots+\lambda_n e_n.
                $\\
                Dunque $Span (e_1,\dots,e_n)=\mathbb{R}^n$ e 
                $\lambda_1 e_1+\dots+\lambda_n e_n=0$ 
                se e solo se \\$\lambda_1=\dots=\lambda_n=0$.
            \subsection{Coordinate}
                \paragraph{}Sia $v_1,\dots,v_n$ una base di V e $v \in V$ un vettore. 
                Allora $$\exists! \; \alpha_1,\dots,\alpha_n : v=\alpha_1 v_1+\alpha_2 v_2+\dots+\alpha_n v_n$$
                ovvero \textbf{ogni vettore} si scrive in un modo \textbf{unico} come \textbf{combinazione lineare} degli \textbf{elementi della base}.\\
                Gli $\boldsymbol{\alpha_i}$ sono le \textbf{coordinate} di v rispetto alla \textbf{base}.
                \paragraph{Trovare le coordinate di un vettore rispetto alla base}\mbox{}\\
                Sappiamo da esercizi precedenti che
                    $
                    v_1=
                    \begin{bmatrix}
                        1\\
                        2\\
                        3
                    \end{bmatrix}
                    ,v_2=
                    \begin{bmatrix}
                        1\\
                        0\\
                        1
                    \end{bmatrix}
                    ,v_3=
                    \begin{bmatrix}
                        0\\
                        0\\
                        1
                    \end{bmatrix}
                    $
                    sono una \textbf{base} di $\mathbb{R}^3$. Trovare le coordinate di
                    $
                    \begin{bmatrix}
                        0\\
                        2\\
                        1
                    \end{bmatrix}
                    $ rispetto a questa base.
                    $$
                    \alpha_1
                    \begin{bmatrix}
                        1\\
                        2\\
                        3
                    \end{bmatrix}
                    +\alpha_2
                    \begin{bmatrix}
                        1\\
                        0\\
                        1
                    \end{bmatrix}
                    +\alpha_3
                    \begin{bmatrix}
                        0\\
                        0\\
                        1
                    \end{bmatrix}
                    =
                    \begin{bmatrix}
                        0\\
                        2\\
                        1
                    \end{bmatrix}
                    $$
                Applichiamo l'\textbf{algoritmo di Gauss-Jordan}.
                $$
                \begin{bmatrix}
                    1 & 1 & 0 & \vline & 0\\
                    2 & 0 & 0 & \vline & 2\\
                    3 & 1 & 1 & \vline & 1
                \end{bmatrix}
                \xrightarrow[R_3-3R_1]{R_2-2R_1}
                \begin{bmatrix}
                    1 & 1 & 0 & \vline & 0\\
                    0 & -2 & 0 & \vline & 2\\
                    0 & -2 & 1 & \vline & 1
                \end{bmatrix}
                \xrightarrow[R_2\rightarrow \frac{1}{2}R_2]{R_3-R_2}
                \begin{bmatrix}
                    1 & 1 & 0 & \vline & 1\\
                    0 & 1 & 0 & \vline & -1\\
                    0 & 0 & 1 & \vline & -1
                \end{bmatrix}
                $$
                $$
                \xrightarrow[]{R_1-R_2}
                \begin{bmatrix}
                    1 & 0 & 0 & \vline & 1\\
                    0 & 1 & 0 & \vline & -1\\
                    0 & 0 & 1 & \vline & -1
                \end{bmatrix}
                $$
                Quindi 
                $
                \begin{cases}
                    \alpha_1=1\\
                    \alpha_2=-1\\
                    \alpha_3=-1
                \end{cases}
                \text{e }
                1
                \begin{bmatrix}
                    1\\
                    2\\
                    3
                \end{bmatrix}
                +-1
                \begin{bmatrix}
                    1\\
                    0\\
                    1
                \end{bmatrix}
                +-1
                \begin{bmatrix}
                    0\\
                    0\\
                    1
                \end{bmatrix}
                =
                \begin{bmatrix}
                    0\\
                    2\\
                    1
                \end{bmatrix}
                $
        \section{Dimensione}
        La \textbf{dimensione} di uno spazio V sarà definita come il \textbf{numero degli elementi di una base}. \textbf{Questo numero è lo stesso per ogni base}. \footnote{Dimostrazione a fine lezione 06.}
            \subsection{Proprietà}
            \paragraph{}Se $dim \; V=n$ e $v_1,\dots,v_r \in V$ i casi sono:
            \begin{itemize}
                \item $\boldsymbol{r>n} \Leftrightarrow v_1,\dots,v_r$ sono \textbf{linearmente dipendenti}
                \item $\boldsymbol{r=n}$ \textbf{e} $\boldsymbol{v_1,\dots,v_n}$ \textbf{linearmente indipendenti} $\Leftrightarrow$ è una \textbf{base}
                \item $\boldsymbol{r<n}$ \textbf{e} $\boldsymbol{v_1,\dots,v_n}$ \textbf{linearmente indipendenti} $\Leftrightarrow$ si \textbf{completa}\footnote{Posso aggiungere vettori affinché diventi una base} in una \textbf{base}
            \end{itemize}
            \paragraph{Esempio}\mbox{}\\
                Decidiamo se 
                    $
                    \begin{bmatrix}
                        1\\
                        -1\\
                        1
                    \end{bmatrix}
                    ,
                    \begin{bmatrix}
                        -1\\
                        1\\
                        -1
                    \end{bmatrix}
                    ,
                    \begin{bmatrix}
                        1\\
                        -1\\
                        1
                    \end{bmatrix}
                    \begin{bmatrix}
                        -1\\
                        -1\\
                        1
                    \end{bmatrix}
                    $ è una base di $\mathbb{R}^3$\\
                    $dim \; \mathbb{R}^3 = 3 \Rightarrow$ \textbf{se} sono \textbf{indipendenti} formano una \textbf{base}.\\
                    Verifichiamo con \textbf{Gauss}:
                    $$
                    \begin{bmatrix}
                        1 & -1 & -1\\
                        -1 & 1 & -1\\
                        1 & -1 & 1
                    \end{bmatrix}
                    \xrightarrow[R_3-R_1]{R_2+R_1}
                    \begin{bmatrix}
                        \textcolor{blue}{1} & -1 & -1\\
                        0 & 0 & -2\\
                        0 & 0 & 2
                    \end{bmatrix}
                    \xrightarrow[]{R_3+R_2}
                    \begin{bmatrix}
                        \textcolor{blue}{1} & -1 & -1\\
                        0 & 0 & \textcolor{blue}{-2}\\
                        0 & 0 & 0
                    \end{bmatrix}                                                
                    $$
                    2 \textbf{pivots} $\Rightarrow$ i vettori sono \textbf{linearmente dipendenti}.\\
                    Però i \textbf{pivots} sono nelle colonne 1,3 quindi escludendo la colonna 2:
                    $$
                    v_1=
                    \begin{bmatrix}
                        1\\
                        -1\\
                        1
                    \end{bmatrix}
                    ,v_2=
                    \begin{bmatrix}
                        -1\\
                        -1\\
                        1
                    \end{bmatrix}
                    \text{sono \textbf{linearmente indipendenti.}}\footnote{Il vettore $v_2$ è il vecchio vettore $v_3$, cambio di notazione per proseguire l'esercizio}
                    $$
                    Ora $dim \; Span(v_1,v_2)=2, dim \; \mathbb{R}^3=3 $.\\
                    Troviamo ora un vettore di $\mathbb{R}^3$ \textbf{non contenuto} nello $Span (v_1,v_2)$.\\
                    Una strategia può essere partire dalla \textbf{base standard}:
                    $
                    \begin{bmatrix}
                        1\\
                        0\\
                        0
                    \end{bmatrix}
                    ,
                    \begin{bmatrix}
                        0\\
                        1\\
                        0
                    \end{bmatrix},
                    \begin{bmatrix}
                        0\\
                        0\\
                        1
                    \end{bmatrix}
                    $
                    Una delle 3 basi standard non è sicuramente contenuta nello $Span(v_1,v_2)$ altrimenti esso sarebbe una base.\\
                    \textbf{Cerchiamo quindi il vettore della base standard che è linearmente indipendente agli altri 2 vettori.}
                    Proviamo con $e_3$:
                    $$
                    \begin{bmatrix}
                        1 & -1 & 0\\
                        -1 & -1 & 0\\
                        1 & 1 & 1
                    \end{bmatrix}
                    \xrightarrow[R_3-R_1]{R_2+R_1}
                    \begin{bmatrix}
                        \textcolor{blue}{1} & -1 & 0\\
                        0 & -2 & 0\\
                        0 & 2 & 1
                    \end{bmatrix}
                    \xrightarrow[]{R_3+R_2}
                    \begin{bmatrix}
                        \textcolor{blue}{1} & -1 & 0\\
                        0 & \textcolor{blue}{-2} & 0\\
                        0 & 0 & \textcolor{blue}{1}
                    \end{bmatrix}
                    $$
                    \textbf{3 pivots} $\Rightarrow e_3$ completa la nostra base. $e_1$ invece \textbf{non la completa}. 
            \paragraph{Proposizione} Sia $W \subset V$ un \textbf{sottospazio}. Allora
                    \begin{enumerate}
                        \item $dim \; W \leq dim \; V$
                        \item Se $W \neq V$, allora $dim \; W< dim \; V$
                    \end{enumerate}
                Questa proposizione è utile per calcolare le dimensioni dei sottospazi.
                \paragraph{Esempio}
                $$
                \text{Sia }
                V=\left \{
                \begin{bmatrix}
                    a & b\\
                    c & d
                \end{bmatrix}
                \in M_{2x2} \; (\mathbb{R}):b=c \right \}
                \text{\textbf{(Matrici simmetriche)}}
                $$\\
                $dim \; M_{2x2} \; (\mathbb{R})=4$ (base standard).\\[4px]
                $V \neq M_{2x2}(\mathbb{R}) \Rightarrow dim \; V \leq 3$.\\[4px]
                Ma 
                $
                \begin{bmatrix}
                    1 & 0\\
                    0 & 0
                \end{bmatrix}
                ,
                \begin{bmatrix}
                    0 & 0\\
                    0 & 1
                \end{bmatrix}
                ,
                \begin{bmatrix}
                    0 & 1\\
                    1 & 0
                \end{bmatrix}
                $ sono \textbf{linearmente indipendenti} $\Rightarrow dim \; V=3$
            \subsection{Sottospazi}
                \subsubsection{Intersezioni di sottospazi}
                \paragraph{}Se $W_1,W_2 \subset V$ sottospazi $\Rightarrow W_1 \cap W_2$ è sottospazio.
                \subsubsection{Formula di Grassmann}
                \paragraph{}Siano $V_1,V_2 \subset V$ due sottospazi allora
                $$V_1+V_2:=\{v_1+v_2 : v_1 \in V_1, v_2 \in V_2 \}$$
            \paragraph{Osservazione}$V_1+V_2$ è un \textbf{sottospazio}.
            \paragraph{Esempio}
                $$
                V_1=\left \{ 
                \begin{bmatrix}
                    0\\
                    a_2\\
                    a_3
                \end{bmatrix}
                :a_2,a_3 \in \mathbb{R}^3    
                \right \}
                ,V_2=\left \{ 
                \begin{bmatrix}
                    a_1\\
                    a_2\\
                    0
                \end{bmatrix}
                :a_1,a_2 \in \mathbb{R}^3    
                \right \} \subset \mathbb{R}^3
                $$
                $V_1+V_2=\mathbb{R}^3$, ma anche $V_1 \cap V_2=\left \{ 
                    \begin{bmatrix}
                    0\\
                        a_2\\
                        0
                    \end{bmatrix}
                    :a_2 \in \mathbb{R}^3    
                \right \}
                $
            \paragraph{Formula di Grassmann} Se $dim < \infty, V_1,V_2 \subset V$ sottospazi allora
            $$dim(V_1+V_2)=dim \; V_1 + dim \; V_2 - dim(V_1 \cap V_2)$$
            \paragraph{Esempio} In $\mathbb{R}^4$ consideriamo i sottospazi
            $$
            V=\left \{
            \text{soluzioni di}
            \begin{cases}
                x_1+2x_2+x_3=0\\
                -x_1-x_2+3x_4=0
            \end{cases}    
            \right \}
            $$
            $$
            W=Span \left (
            v_1=
            \begin{bmatrix}
                2\\
                0\\
                1\\
                1
            \end{bmatrix}
            ,v_2=
            \begin{bmatrix}
                3\\
                -2\\
                -2\\ 
                0
            \end{bmatrix}
            \right )
            $$
            Calcoliamo $dim(V \cap W )$, $dim(V+W)$
            \paragraph{Soluzione}\mbox{}\\
            $\boldsymbol{dim \; W=2}$ perché ovviamente $W_1 \neq \lambda W_2$.\\
            Calcoliamo $\dim \; V$
            $$
            \begin{bmatrix}
                1 & 2 & 1 & 0\\
                -1 & -1 & 0 & 3
            \end{bmatrix}
            \rightarrow
            \begin{bmatrix}
                \textcolor{blue}{1} & 2 & 1 & 0\\
                0 & \textcolor{blue}{1} & 1 & 3
            \end{bmatrix}
            \rightarrow
            \begin{cases}
                x_1+2x_2+x_3=0\\
                x_2+x_3+3x_4=0
            \end{cases}
            $$
            $
            x_3,x_4 \text{ variabili libere} \rightarrow
            \begin{cases}
                x_2=-x_3-3x_4\\
                x_1=-2(-x_3-3x_4)-x_3=x_3+6x_4
            \end{cases}
            $\\
            $
            \text{Soluzione generale}
            \begin{bmatrix}
                x_3+6x_4\\
                -x_3-3x_4\\
                x_3\\
                x_4
            \end{bmatrix}
            $
            \\
            Posso scrivere in forma parametrizzata 
            $
            x_3
            \begin{bmatrix}
                1\\
                -1\\
                1\\
                0
            \end{bmatrix}
            +x_4
            \begin{bmatrix}
                6\\
                -3\\
                0\\
                1
            \end{bmatrix}
            $
            e ora sappiamo che $\boldsymbol{dim \; V=2 \text{ e } v_1, v_2}$\textbf{ è una base}.\\
            Cerchiamo ora $dim(V+W)$.\\
            $V+W=Span(v_1,v_2,w_1,w_2)$\\
            Troviamo una base con \textbf{Gauss}:
            $$
            \begin{bmatrix}
                1 & 6 & 2 & 3\\
                -1 & -3 & 0 & -2\\
                1 & 0 & 1 & -2\\
                0 & 1 & 1 & 0
            \end{bmatrix}
            \xrightarrow[R_3-R_1]{R_2+R_1}
            \begin{bmatrix}
                1 & 6 & 2 & 3\\
                0 & 3 & 2 & 1\\
                0 & -6 & -1 & -5\\
                0 & 1 & 1 & 0
            \end{bmatrix}
            \xrightarrow[]{R_3-2R_1}
            \begin{bmatrix}
                1 & 6 & 2 & 3\\
                0 & 3 & 2 & 1\\
                0 & 0 & 3 & -3\\
                0 & 1 & 1 & 0
            \end{bmatrix}
            $$
            $$
            \xrightarrow[]{R_2\circlearrowright R_4}
            \begin{bmatrix}
                1 & 6 & 2 & 3\\
                0 & 1 & 1 & 0\\
                0 & 0 & 3 & -3\\
                0 & 3 & 2 & 1
            \end{bmatrix}
            \xrightarrow[]{R_4-3R_2}
            \begin{bmatrix}
                1 & 6 & 2 & 3\\
                0 & 1 & 1 & 0\\
                0 & 0 & 3 & -3\\
                0 & 0 & -1 & 1
            \end{bmatrix}
            \xrightarrow[]{R_4+\frac{1}{3}R_3}
            \begin{bmatrix}
                \textcolor{blue}{1} & 6 & 2 & 3\\
                0 & \textcolor{blue}{1} & 1 & 0\\
                0 & 0 & \textcolor{blue}{3} & -3\\
                0 & 0 & 0 & 0
            \end{bmatrix}
            $$
            \textbf{3 pivots} quindi le prime 3 colonne sono indipendenti.\\
            Quindi $\boldsymbol{dim \; Span(v_1,v_2,w_1,w_2)=dim(V+W)=3}$.\\
            \textbf{Grassmann:} $dim (V \cap W) = dim \; V + dim \; W - dim(V+W)=2+2-3=1$\\[4px]
            Potevamo anche calcolare direttamente $dim(V\cap W)$:
            $$
            Y\cap W = \left \{ \lambda_1
            \begin{bmatrix}
                2\\
                0\\
                1\\
                1
            \end{bmatrix}    
            +\lambda_2
            \begin{bmatrix}
                3\\
                -2\\
                -2\\
                0
            \end{bmatrix}
            \text{ che soddisfano }
            \begin{cases}
                x_1+2x_2+x_3=0\\
                -x_1-x_2+3x_4=0
            \end{cases}
            \right \}
            $$
            Sostituiamo e otteniamo:
            $$
            \begin{cases}
                (2\lambda_1+3\lambda_2)+2(-2\lambda_2)+(\lambda_1-2\lambda_2)=0\\
                -(2\lambda_1+3\lambda_2)+(-2\lambda_2)+3\lambda_1=0
            \end{cases}\\
            $$
            $$
            \begin{cases}
                3\lambda_1-3\lambda_2=0\\
                \lambda_1-\lambda_2=0
            \end{cases}
            $$
            $$
            \lambda_1=\lambda_2 \Rightarrow dim (V \cap W)=1 \text{ perché } V\cap W=\{\lambda(w_1+w_2):\lambda \in \mathbb{R}\}
            $$
        \section{Rango}
            \paragraph{}Se 
            $
            A=
            \begin{bmatrix}
                a_{11} & \dots & a_{1n}\\
                \vdots\\
                a_{n1} & \dots & a_{mn}                
            \end{bmatrix}
            $
            , il \textbf{rango} di A è
            $$
            rg(A):=dim \; Span \left (
                \begin{bmatrix}
                    a_{11}\\
                    \vdots\\
                    a_{m1}
                \end{bmatrix}
            , ... ,
                \begin{bmatrix}
                    a_{1n}\\
                    \vdots\\
                    a_{mn}
                \end{bmatrix}
            \right )
            $$
            \subsection{Trovare il rango}
            \paragraph{}Per calcolare $rg(A)$ bisogna: 
            \begin{itemize}
                \item estrarre una base di $Span(\text{colonne})$.
                \item usare l'algoritmo di Gauss sulla matrice A
            \end{itemize}
            Se numero colonne linearmente indipendenti = numero dei pivots della forma a scalini $\Rightarrow \boldsymbol{rg(A)=} \text{\textbf{numero di pivot nella forma a scalini}.}$

    \chapter{Applicazioni lineari}
        \paragraph{Definizione}Siano $V_1,V_2$ spazi vettoriali su $\mathbb{R}$. Un'\textbf{applicazione lineare} è una funzione $\varphi: V_1 \rightarrow V_2$ che soddisfa:
            \begin{itemize}
                \item $\varphi (v_1+v_2)=\varphi(v_1)+\varphi(v_2) \; \forall v_1,v_2 \in V_1$
                \item $\lambda \varphi(v)=\varphi(\lambda v) \; \forall \lambda \in \mathbb{R}, \forall v \in V_1$
            \end{itemize}
        \clearpage
        \section{Kernel}
            \paragraph{}Il \textbf{Kernel o nucleo} è un sottospazio:    
            $$Ker(\varphi):= \{v \in V_1 : \varphi(v)=0\}$$
            \paragraph{Proposizione}$Ker(\varphi_1) \subset V_1$ è un sottospazio.
            \subsection{Trovare il Kernel utilizzando la matrice}
            $$
            \text{Se } A=
            \begin{bmatrix}
                a_{11} & \dots & a_{1n}\\
                \vdots\\
                a_{n1} & \dots & a_{mn}
            \end{bmatrix}
            \text{, } v=
            \begin{bmatrix}
                x_1\\
                \vdots\\
                x_n
            \end{bmatrix}
            $$
            $$
            v \in Ker(\varphi) \Leftrightarrow Av=0 \Leftrightarrow 
            \begin{bmatrix}
                x_1\\
                \vdots\\
                x_n
            \end{bmatrix}
            \text{è soluzione di}
            \begin{cases}
                a_{11}x_1+\dots+a_{1n}x_n=0\\
                \vdots\\
                a_{m1}x_1+\dots+a_{mn}x_n=0
            \end{cases}
            $$
            Quindi per trovare $Ker(\varphi)$ bisogna \textbf{risolvere il sistema omogeneo} (ad esempio con Gauss).
            \paragraph{Esempio} Sia $\varphi: \mathbb{R}^4\rightarrow \mathbb{R}^3$ della matrice
            $
            \begin{bmatrix}
                1 & 2 & -1 & -2\\
                1 & 1 & 1 & 1\\
                1 & 0 & 3 & 4
            \end{bmatrix}
            $
            Trovare $Ker(\varphi)$.
            Applichiamo Gauss alla matrice di $\varphi$.
            $$
            \rightarrow
            \begin{bmatrix}
                1 & 2 & -1 & -2\\
                0 & -1 & 2 & 3\\
                0 & -2 & 4 & 6
            \end{bmatrix}
            \rightarrow
            \begin{bmatrix}
                \textcolor{blue}{1} & 2 & -1 & -2\\
                0 & \textcolor{blue}{-1} & 2 & 3\\
                0 & 0 & 0 & 0
            \end{bmatrix}
            $$
            $Ker(\varphi) \Leftrightarrow$ soluzioni di 
            $
            \begin{cases}
                x_1+2x_2-3x_3-2x_4=0\\
                -x_2+2x_3+3x_4=0
            \end{cases}
            $
            \\
            $
            \Rightarrow
            \begin{cases}
                x_2=2x_3+3x_4\\
                x_1=-3x_3-4x_4
            \end{cases}
            \Rightarrow
            \begin{bmatrix}
                -3\\
                2\\
                1\\
                0
            \end{bmatrix}
            x_3
            \begin{bmatrix}
                -4\\
                3\\
                0\\
                1
            \end{bmatrix}
            x_4
            $ soluzione generale del sistema.\\
            Quindi
            $
            \begin{bmatrix}
                -3\\
                2\\
                1\\
                0
            \end{bmatrix}
            x_3
            \begin{bmatrix}
                -4\\
                3\\
                0\\
                1
            \end{bmatrix}
            x_4
            $
            è la \textbf{base} di $Ker(\varphi)$.
        \section{Immagine}
            \paragraph{}L'\textbf{immagine} è un sottospazio:
            $$Im(\varphi):=\{w \in V_2: \exists v \in V_1 \text{ tale che }w=\varphi(v)\}$$
            \paragraph{Proposizione}$Ker(\varphi) \subset V_2$ è un sottospazio.
            \subsection{Trovare l'immagine utilizzando la matrice}
                \paragraph{}Sappiamo che se $e_1,\dots,e_n$ è la base standard,
                $$
                \varphi(e_1)=
                \begin{bmatrix}
                    a_{11}\\
                    \vdots\\
                    a_{m1}
                \end{bmatrix}
                ,\dots, \;
                \varphi(e_n)=
                \begin{bmatrix}
                    a_{1n}\\
                    \vdots\\
                    a_{mn}
                \end{bmatrix}
                $$
            Ma $Im(\varphi) = Span(\varphi(e_1),\dots,\varphi(e_n))$\\
            Quindi $\boldsymbol{Im(\varphi)}$ è lo \textbf{span delle colonne} di A in $\mathbb{R}^m$            
            \subsection{Trovare la dimensione}
            \paragraph{}Per trovare la $\boldsymbol{dim \; Im(\varphi)}$ bisogna determinare la dimensione dello \textbf{span}, ovvero il \textbf{rango}.\\
            Se $\varphi \text{ ha matrice } A$ allora $\boldsymbol{dim \; Im(\varphi)=rg \; (A)}$
            \paragraph{Esempio} Sia $\varphi: \mathbb{R}^4\rightarrow \mathbb{R}^3$ della matrice
            $
            \begin{bmatrix}
                1 & 2 & -1 & -2\\
                1 & 1 & 1 & 1\\
                1 & 0 & 3 & 4
            \end{bmatrix}
            $
            Trovare $Im(\varphi)$.\\
            Applichiamo Gauss alla matrice di $\varphi$.
            $$
            \rightarrow
            \begin{bmatrix}
                1 & 2 & -1 & -2\\
                0 & -1 & 2 & 3\\
                0 & -2 & 4 & 6
            \end{bmatrix}
            \rightarrow
            \begin{bmatrix}
                \textcolor{blue}{1} & 2 & -1 & -2\\
                0 & \textcolor{blue}{-1} & 2 & 3\\
                0 & 0 & 0 & 0
            \end{bmatrix}
            $$
            Ci sono \textbf{pivots} nelle prime due colonne $\Rightarrow
            \begin{bmatrix}
                1\\
                1\\
                1
            \end{bmatrix}
            ,
            \begin{bmatrix}
                2\\
                1\\
                0
            \end{bmatrix}
            \text{ è la base di }Im(\varphi)$ e il \textbf{rango} è 2.
        \section{Dimensione}
            \paragraph{Teorema} Se $dim \; V_1<\infty$ allora
            $$dim \; Ker(\varphi)+dim \; Im(\varphi)=dim \; V_1$$
            in $\varphi:V_1\rightarrow V_2$. La dimensione di $V_2$ non riguarda questo teorema.
        \section{Prodotto}
            \paragraph{}Se $A \in M_{mxn}(\mathbb{R}), v \in \mathbb{R}^n$, il loro \textbf{prodotto} è il vettore in $\mathbb{R}^m$:
            $$
            A=
            \begin{bmatrix}
                a_{11} & \dots & a_{1n}\\
                \vdots & \vdots & \vdots\\
                a_{m1} & \dots & a_{mn}
            \end{bmatrix}
            \cdot 
            \begin{bmatrix}
                b_1\\
                \vdots\\
                b_n
            \end{bmatrix}
            :=
            \begin{bmatrix}
                a_{11}b_1+a_{12}b_2+\dots+a_{1n}b_n\\
                a_{21}b_1+a_{22}b_2+\dots+a_{2n}b_n\\
                \vdots\\
                a_{m1}b_1+a_{m2}b_2+\dots+a_{mn}b_n
            \end{bmatrix}
            \in \mathbb{R}^m
            $$
            Il vettore moltiplicato deve avere lo stesso numero di colonne della matrice.
            \paragraph{Proposizione} Se $v=
            \begin{bmatrix}
                b_1\\
                \vdots\\
                b_n
            \end{bmatrix}
            $
            è un vettore generale, allora $\varphi(v)=A\cdot v$.
            \paragraph{Esempio 1}Sia $\varphi:\mathbb{R}^3\rightarrow R,\; \varphi \left (
                \begin{bmatrix}
                    x\\
                    y\\
                    z
                \end{bmatrix}
            \right )=x+2y+3z$. Trovare 
            $
            \varphi \left (
            \begin{bmatrix}
                1\\
                -1\\
                1
            \end{bmatrix}
            \right )
            $.\\
            Naturalmente $1\cdot 1+-2\cdot 1+1\cdot 3=2$. Ma anche:
            $$
            \varphi \left ( 
            \begin{bmatrix}
                1\\
                0\\
                0\\
            \end{bmatrix}    
            \right )
            =1,\;
            \varphi \left ( 
            \begin{bmatrix}
                0\\
                1\\
                0\\
            \end{bmatrix}    
            \right )
            =2,\;
            \varphi \left ( 
            \begin{bmatrix}
                0\\
                0\\
                1\\
            \end{bmatrix}    
            \right )
            =3.\;
            $$
            $$
            \varphi \left ( 
            \begin{bmatrix}
                1\\
                -1\\
                1\\
            \end{bmatrix}    
            \right )
            =
            \begin{bmatrix}
                1 & 2 & 3
            \end{bmatrix}
            \cdot
            \begin{bmatrix}
                1\\
                -1\\
                1\\
            \end{bmatrix}
            =1\cdot 1+2\cdot -1+1 \cdot 3=2 
            $$
            \paragraph{Esempio 2}$\varphi:\mathbb{R}^2\rightarrow \mathbb{R}^2, \;
            \varphi \left(
            \begin{bmatrix}
                x\\
                y
            \end{bmatrix}
            \right)
            =
            \left(
            \begin{bmatrix}
                x+y\\
                x-y    
            \end{bmatrix}
            \right)
            $
            $$
            \varphi \left ( 
                \begin{bmatrix}
                    1\\
                    0\\
                \end{bmatrix}    
                \right )
                =
                \begin{bmatrix}
                    1\\
                    1
                \end{bmatrix},\;
                \varphi \left ( 
                \begin{bmatrix}
                    0\\
                    1\\
                \end{bmatrix}    
                \right )
                =
                \begin{bmatrix}
                    1\\
                    -1
                \end{bmatrix}.
            $$
            $$
            \begin{bmatrix}
                1 & 1\\
                1 & -1
            \end{bmatrix}
            \cdot
            \begin{bmatrix}
                b_1\\
                b_2
            \end{bmatrix}
            =
            \begin{bmatrix}
                b_1+b_2\\
                b_1-b_2
            \end{bmatrix}
            \text{ Vettore generico}
            $$
            \paragraph{Conclusione} Se $\varphi:\mathbb{R}^n\rightarrow \mathbb{R}^m$ lineare, $\varphi(e_1),...,\varphi(e_n)$ base standard allora:
            \begin{itemize}
                \item $\varphi(e_1),\dots,\varphi(e_n)$ \textbf{determina} $\boldsymbol{\varphi}$\textbf{ in maniera unica}
                \item $\forall v$ possiamo calcolare $\varphi(v)=A \cdot v$ dove $A \in M_{mxn} (\mathbb{R})$ è la matrice definita nel punto precedente.
            \end{itemize}
        \section{Matrice associata all'applicazione lineare}
            \paragraph{}Sia $\varphi:V \rightarrow W$ lineare.\\
            Sia $B=\{e_1,\dots,e_n\}$ una base di V [dim V = n]\\
            Sia $B'=\{e_1',\dots,e_m'\}$ una base di W [dim W = m]\\
            Scriviamo
            $$
            \begin{matrix}
                \varphi(e_1)=a_{11}e'_1+a_{21}e'_2+\dots+a_{m1}e'_m\\
                \varphi(e_2)=a_{12}e'_1+a_{22}e'_2+\dots+a_{m2}e'_m\\
                \vdots\\
                \varphi(e_n)=a_{1n}e'_1+a_{2n}e'_2+\dots+a_{mn}e'_m
            \end{matrix}
            $$
            \textbf{La matrice di} $\boldsymbol{\varphi}$ \textbf{rispetto alla base} $\boldsymbol{B,B'}$ \textbf{è}:
            $$
            A=
            \begin{bmatrix}
                a_{11} & a_{12} & \dots & a_{1n}\\
                a_{21} & a_{22} & \dots & a_{2n}\\
                \vdots\\
                a_{m1} & a_{m2} & \dots & a_{mn}
            \end{bmatrix}
            \in M_{mxn}(\mathbb{R})
            $$
            Quindi 
            $$
            A=
            \begin{bmatrix}
                \varphi(e_1)& \vline & \varphi(e_2)& \vline & \dots & \vline & \varphi(e_n)
            \end{bmatrix}
            $$
            dove le \textbf{colonne sono le coordinate di} $\boldsymbol{\varphi(e_i)}$ rispetto a $e_1',\dots,e_m'$.
            \paragraph{Teorema} Se $v=b_1e_1+\dots+b_ne_n$ è un vettore di V consideriamo il vettore
            colonna in $\mathbb{R}^n: \;
            \begin{bmatrix}
                b_1\\
                \vdots\\
                b_n
            \end{bmatrix}
            $.\\
            Allora le coordinate di $\varphi(v)$ rispetto a $B'=\{e_1',\dots,e_m'\}$ sono date dal vettore colonna
            $$
            A \cdot
            \begin{bmatrix}
                b_1\\
                \vdots\\
                b_n    
            \end{bmatrix}
            \in \mathbb{R}^m
            $$
            \paragraph{Importante} La matrice A è sempre definita con \textbf{due basi} $\boldsymbol{B,B'}$.
            \paragraph{Esempio}$\varphi: \mathbb{R}^2 \rightarrow \mathbb{R}^2, \; 
            \varphi \left(
                \begin{bmatrix}
                    x\\
                    y
                \end{bmatrix}
            \right)=\left(
            \begin{matrix}
                x+2y\\
                x+2y
            \end{matrix}\right)
            $\\
            Matrice rispetto alla base standard:
            $$
            \varphi \left(
                \begin{bmatrix}
                    1\\
                    0
                \end{bmatrix}
            \right)=
            \begin{bmatrix}
                1\\
                1
            \end{bmatrix}
            , \varphi \left(
                \begin{bmatrix}
                    0\\
                    1
                \end{bmatrix}
            \right)=
            \begin{bmatrix}
                2\\
                2
            \end{bmatrix}
            \Rightarrow
            A=
            \begin{bmatrix}
                1 & 2\\
                1 & 2
            \end{bmatrix}
            $$
            Matrice rispetto alla base 
            $
            \begin{bmatrix}
                1\\
                0
            \end{bmatrix}
            ,
            \begin{bmatrix}
                1\\
                1
            \end{bmatrix}
            \text{di } \mathbb{R}^2
            $
            $$
            \varphi \left(
                \begin{bmatrix}
                    1\\
                    0
                \end{bmatrix}
            \right)=
            \begin{bmatrix}
                1\\
                1
            \end{bmatrix}
            =\textcolor{blue}{0} \cdot
            \begin{bmatrix}
                1\\
                0
            \end{bmatrix}
            +\textcolor{orange}{1}\cdot
            \begin{bmatrix}
                1\\
                1
            \end{bmatrix}
            $$
            $$
            \varphi \left(
                \begin{bmatrix}
                    1\\
                    1
                \end{bmatrix}
            \right)=
            \begin{bmatrix}
                3\\
                3
            \end{bmatrix}
            =\textcolor{blue}{0} \cdot
            \begin{bmatrix}
                1\\
                0
            \end{bmatrix}
            +\textcolor{orange}{3}\cdot
            \begin{bmatrix}
                1\\
                1
            \end{bmatrix}
            $$
            $$
            A=
            \begin{bmatrix}
                \textcolor{blue}{0} & \textcolor{orange}{0}\\  
                \textcolor{blue}{1} & \textcolor{orange}{3}    
            \end{bmatrix}
            $$
            Matrice rispetto alla base 
            $
            \begin{bmatrix}
                1\\
                1
            \end{bmatrix}
            ,
            \begin{bmatrix}
                2\\
                -1
            \end{bmatrix}
            \text{di } \mathbb{R}^2
            $
            $$
            \varphi \left(
                \begin{bmatrix}
                    1\\
                    1
                \end{bmatrix}
            \right)=
            \begin{bmatrix}
                3\\
                3
            \end{bmatrix}
            =\textcolor{blue}{3} \cdot
            \begin{bmatrix}
                1\\
                1
            \end{bmatrix}
            +\textcolor{orange}{0}\cdot
            \begin{bmatrix}
                2\\
                -1
            \end{bmatrix}
            $$
            $$
            \varphi \left(
                \begin{bmatrix}
                    2\\
                    -1
                \end{bmatrix}
            \right)=
            \begin{bmatrix}
                0\\
                0
            \end{bmatrix}
            =\textcolor{blue}{0} \cdot
            \begin{bmatrix}
                1\\
                1
            \end{bmatrix}
            +\textcolor{orange}{0}\cdot
            \begin{bmatrix}
                2\\
                -1
            \end{bmatrix}
            $$
            $$
            A=
            \begin{bmatrix}
                \textcolor{blue}{3} & \textcolor{orange}{0}\\  
                \textcolor{blue}{0} & \textcolor{orange}{0}    
            \end{bmatrix}
            $$
            Quindi scrivendo le coordinate \textbf{non} rispetto alla base standard ma ad \textbf{altre
            basi}, $\varphi$ può diventare molto più semplice. Per trovare \textbf{basi ottimali} si utilizzeranno
            gli \textbf{autovettori}.
        \clearpage
        \section{Isomorfismo}
        \paragraph{} Un'applicazione lineare $\varphi: V_1 \rightarrow V_2$ è un \textbf{isomorfismo} se 
        \begin{itemize}
            \item $Im(\varphi)=V_2$ 
            \item per $v_1,v_1' \in V_1 \; \varphi(v_1)=\varphi(v_1') \Leftrightarrow v_1=v_1'$. Notazione: $v_1 \tilde{\rightarrow} v_2$
        \end{itemize}
        Dunque $\varphi$ è un \textbf{isomorfismo} se $\boldsymbol{\forall \; v_2 \in V_2 \; \exists! v_1 \in V_1:\varphi(v_1)=v_2}$.
        \paragraph{Osservazione} $\varphi$ è un isomorfismo $\Leftrightarrow Im(\varphi)=V_2, \; Ker(\varphi)=V_1$.
        \paragraph{Esempio} $V_1=M_{2x2}(\mathbb{R}), V_2=\mathbb{R}^4 \; \; \varphi:M_{2x2}(\mathbb{R})\rightarrow \mathbb{R}^4$
        $$
        \begin{bmatrix}
            a & b\\
            c & d
        \end{bmatrix}
        \rightarrow
        \begin{bmatrix}
            a\\
            b\\
            c\\
            d
        \end{bmatrix}
        \text{è un \textbf{isomorfismo}.}
        $$
        $$
        \text{Ma anche }
        \begin{bmatrix}
            a\\
            b\\
            c\\
            d
        \end{bmatrix}
        \rightarrow
        \begin{bmatrix}
            a & b\\
            c & d
        \end{bmatrix}
        \text{definisce un \textbf{isomorfismo}. }
        \psi : \mathbb{R}^4 \rightarrow M_{2x2}(\mathbb{R}).
        $$
        \paragraph{Osservazione} Sia V uno spazio vettoriale, $dim \; V=n$ e $B=(e_1,\dots,e_n)$ una base di V.
        Ogni $v \in V$ si scrive in modo unico come $v=a_1e_1+\dots+a_ne_n$.
        Allora $v=
        \begin{bmatrix}
            a_1\\
            \vdots\\
            a_n
        \end{bmatrix}
        $ definisce un \textbf{isomorfismo} $V \tilde{\rightarrow}\mathbb{R}^n$.\\
        Infatti $\varphi$ è lineare, $Im(\varphi)=\mathbb{R}^4, Ker(\varphi)=\{0\}$.
        \paragraph{Osservazione} Sia $\varphi:V_1 \rightarrow V_2$ una qualsiasi applicazione lineare,\\
        $dim \; V_1=n,\; dim \; V_2=m$.\\
        $B=(e_1,\dots,e_n)$ base di $V_1$.\\
        $B'=(e_1',\dots,e_m')$ base di $V_2$.\\
        Sia A la matrice di $\varphi$ rispetto a $B \text{ e } B'$.\\
        B definisce $\psi: V_1 \tilde{\rightarrow}\mathbb{R}^n$.\\
        $B'$ definisce $\psi': V_2 \tilde{\rightarrow}\mathbb{R}^m$.
        \subparagraph{Lemma}
        \begin{itemize}
            \item $\psi^-1$ induce un \textbf{isomorfismo} $Ker(v\mapsto Av)\tilde{\rightarrow}Ker(\varphi)$
            \item $\psi'$ induce un \textbf{isomorfismo} $Ker(\varphi)\tilde{\rightarrow}Ker(v\mapsto Av)$
        \end{itemize}
        \section{Composizione di funzioni}
        \paragraph{} Siano $V_1 \xrightarrow[]{\varphi}V_2\xrightarrow[]{\psi}V_3$ applicazioni lineari.\\
        Allora $\psi \circ \varphi: V_1 \rightarrow V_3$ è \textbf{lineare}.\\
        Siano\\ 
        $
        B_1=(e_1,\dots,e_n) \text{ base di } V_1
        $\\$
        B_2=(f_1,\dots,f_n) \text{ base di } V_2
        $\\$        
        B_3=(g_1,\dots,g_n) \text{ base di } V_3
        $
        \\Siano\\
        $
        B= \text{ matrice di } \varphi \text{ rispetto a } B_1,B_2
        $\\$
        A= \text{ matrice di } \psi \text{ rispetto a } B_2,B_3
        $\\
        Allora la matrice di $\boldsymbol{\psi \circ \varphi}$ rispetto a $B_1,B_3$ è $\boldsymbol{A \cdot B}$. 
        \section{Proprietà della moltiplicazione}
        \begin{enumerate}
            \item $A(BC) = (AB)C$
            \item $A\cdot B =B \cdot A$ \textbf{non è vero in generale}
            \item Sia $I=
            \begin{bmatrix}
                1 & & 0\\
                &\ddots\\
                0 & & 1
            \end{bmatrix}
            $ la matrice \textbf{identità} di $M_{mxn}(\mathbb{R})$. Allora\\
            $\forall A \in M_{nxn}(\mathbb{R}): \; \boldsymbol{I \cdot A = A \cdot I = A}$
            \item Se $\varphi : V_1 \tilde{\rightarrow}V_2$ isomorfismo $\Rightarrow \exists \; \varphi^{-1}:V_2 \tilde{\rightarrow}V_1$
            tale che $\varphi \circ \varphi^{-1}= id_{V_2}$ e $\varphi^{-1} \circ \varphi= id_{V_1}$\footnote{id=identità}.\\
            Se fissiamo le basi \\$B_1$ di $V_1,$ $B_2 $ di $V_2,$ $\varphi$ ha matrice $A$ e $\varphi^{-1}$ ha matrice $A^{-1} \Rightarrow 
            $ $$\boldsymbol{A \cdot A^{-1} = I$, $A^{-1} \cdot A =I}$$ 
            \item \textbf{Matrice inversa:} Esiste \textbf{solo} se è la matrice di $\varphi$ isomorfismo.
        \end{enumerate}
        %RIFLESSIONI GEOMETRIA PIANA FINE LEZIONE 13
        \clearpage
        \section{Cambiamento di base}
        \paragraph{}Sia V uno spazio vettoriale di dim n e $B, B'$ due basi.\\
        La \textbf{matrice di cambiamento di base} rispetto a ($B,B'$) è definita da\footnote{$id_V$ che va da B a $B'$} $$\boldsymbol{P:=[id_V]^{B'}_B}$$
        \paragraph{Esempio} Siano $V=\mathbb{R}^2, B=\left \{
        \begin{bmatrix}
            1\\
            0
        \end{bmatrix},
        \begin{bmatrix}
            0\\
            1
        \end{bmatrix}
        \right\}
        ,B'=
        \left\{
        \begin{bmatrix}
            2\\
            1    
        \end{bmatrix}
        ,\begin{bmatrix}
            1\\
            1
        \end{bmatrix}
        \right\} \\ \varphi: v \mapsto A \cdot v$, dove $A=
        \begin{bmatrix}
            2 & -2\\
            1 & -1
        \end{bmatrix}$ Allora:\\
        $[\varphi]_B=A$, calcoliamo $[\varphi]_{B'}$:
        $$
        \varphi\left(
            \begin{bmatrix}
                2\\
                1
            \end{bmatrix}\right)=
            \begin{bmatrix}
                2 & -2\\
                1 & -1
            \end{bmatrix}\cdot
            \begin{bmatrix}
                2\\
                1
            \end{bmatrix}=
            \begin{bmatrix}
                2\\
                1
            \end{bmatrix}
            =1 \cdot
            \begin{bmatrix}
                2\\
                1
            \end{bmatrix}
            +0\cdot
            \begin{bmatrix}
                1\\
                1
            \end{bmatrix}
        $$
        $$
        \varphi\left(
            \begin{bmatrix}
                1\\
                1
            \end{bmatrix}\right)=
            \begin{bmatrix}
                2 & -2\\
                1 & -1
            \end{bmatrix}\cdot
            \begin{bmatrix}
                1\\
                1
            \end{bmatrix}=
            \begin{bmatrix}
                0\\
                0
            \end{bmatrix}
            =0 \cdot
            \begin{bmatrix}
                2\\
                1
            \end{bmatrix}
            +0\cdot
            \begin{bmatrix}
                1\\
                1
            \end{bmatrix}
        $$
        Quindi $[\varphi]_{B'}=
        \begin{bmatrix}
            1 & 0\\
            0 & 0
        \end{bmatrix}$\\
        \textbf{Calcoliamo ora} $\boldsymbol{P=[id_V]^{B'}_B}$\\
        Calcoliamo con Gauss la \textbf{matrice del cambiamento di base}.
        Scriviamo la base B usando le coordinate della base $B'$.
        $$
        \begin{bmatrix}
            1\\
            0
        \end{bmatrix}
        = 1 \cdot 
        \begin{bmatrix}
            2\\
            1
        \end{bmatrix}
        +(-1)\cdot
        \begin{bmatrix}
            1\\
            1
        \end{bmatrix}
        $$
        $$
        \begin{bmatrix}
            0\\
            1
        \end{bmatrix}
        = (-1) \cdot 
        \begin{bmatrix}
            2\\
            1
        \end{bmatrix}
        +2\cdot
        \begin{bmatrix}
            1\\
            1
        \end{bmatrix}
        $$
        $$\boldsymbol{P=
        \begin{bmatrix}
            1 & -1\\
            -1 & 2
        \end{bmatrix}} \text{ è la \textbf{matrice del cambio di base} da \textbf{B} a }\boldsymbol{B'} $$
        $$
        \footnote{Usando lo stesso metodo ma è banale poiché B, ovvero la base di destinazione, è la matrice identità}
        \boldsymbol{
        \begin{bmatrix}
            2 & 1\\
            1 & 1
        \end{bmatrix}}
        \text{ è la \textbf{matrice di cambio di base} da } \boldsymbol{B'} \text{ a \textbf{B}}  
        $$
        Si nota che:
        $$
        \begin{bmatrix}
            2 & 1\\
            1 & 1
        \end{bmatrix}
        \cdot
        \begin{bmatrix}
            1 & -1\\
            -1 & 2
        \end{bmatrix}
        =
        \begin{bmatrix}
            1 & 0\\
            0 & 1
        \end{bmatrix}
        $$
        \paragraph{In generale} $[id_V]^B_{B'}= [id_V]^{B'}_B=[id_V \circ id_V]_B=[id_V]_B$ è la \textbf{matrice identità}.\\
        Quindi $\boldsymbol{P=[id_V]^{B'}_B}$ è \textbf{invertibile} e $\boldsymbol{P^{-1}=[id_V]^B_{B'}}$
        \paragraph{Teorema} Se $B, B'$ sono due basi di V, P matrice di cambiamento di base.
        $\varphi: V \rightarrow V$ applicazione lineare con $A=[\varphi]_B$ allora $[\varphi]_{B'}=PAP^{-1}$
        \subparagraph{Esempio} Con l'esempio precedente:
        $$
        PAP^{-1}= 
        \begin{bmatrix}
            1 & -1\\
            -1 & 2
        \end{bmatrix}
        \begin{bmatrix}
            2 & -2\\
            1 & -1
        \end{bmatrix}
        \begin{bmatrix}
            2 & 1\\
            1 & 1
        \end{bmatrix}
        =\begin{bmatrix}
            1 & 0\\
            0 & 0
        \end{bmatrix}
        $$
    \section{Determinante}
    \paragraph{} Il \textbf{determinante} sarà un funzione $\boldsymbol{det:M_{nxn}(\mathbb{R})\rightarrow \mathbb{R}}$ con la proprietà fondamentale:
    \begin{itemize}
        \item $det(A)=0 \Leftrightarrow$ le righe di A sono \textbf{linearmente indipendenti}
        \item $det(A)=0 \Leftrightarrow$ le colonne di A sono \textbf{linearmente indipendenti}
    \end{itemize}
    \paragraph{Esempi}
    \begin{itemize}
        \item $n=1 \;\; A=[a] \;\; det(A)=a$
        \item $n=2 \;\; A=
        \begin{bmatrix}
            a & b\\
            c & d
        \end{bmatrix}
        \;\; det(A)=ad- bc$
    \end{itemize}
    \paragraph{Geometricamente} 
    $$
    det
    \begin{bmatrix}
        a & b\\
        c & d
    \end{bmatrix}
    = \text{\textbf{area del parallelogramma} costruito sui lati }
    \begin{bmatrix}
        a\\
        c
    \end{bmatrix}
    ,
    \begin{bmatrix}
        b\\
        d
    \end{bmatrix}
    $$
    %GEOMETRIE E DIMOSTRAZIONI LEZIONE 15
    \paragraph{Definizione generale} Per induzione su n-1, sapendo n=1,2:\\
    Per $A=[a_{ij}] \text{ sia } A_{ij}$ la matrice ottenuta cancellando la riga i e la colonna j:
        \begin{itemize}
            \item $\sum_{j} \; (-1)^{i+j} \; a_{ij} \; det(A_{ij}) \; \;$per i fisso (\textbf{sviluppo secondo la riga i})
            \item $\sum_{i} \; (-1)^{i+j} \; a_{ij} \; det(A_{ij}) \; \;$per j fisso (\textbf{sviluppo secondo la colonna j})
        \end{itemize}
    \textbf{Il risultato sarà uguale per entrambe le formule} e per qualsiasi i,j scelto.
    \paragraph{Promemoria per i segni}
    $$
    (-1)^{i+j}:
    \begin{bmatrix}
        + & - & + & - & + & - & \dots\\
        - & + & - & + & - & + & \dots\\
        + & - & + & - & + & - & \dots\\
        - & + & - & + & - & + & \dots\\
        \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
    \end{bmatrix}
    $$
    \paragraph{Esempio} Per n=3:
    $$
    A=
    \begin{bmatrix}
        1 & 0 & -1\\
        2 & 1 & 0\\
        -1 & 0 & 2
    \end{bmatrix}
    $$
    Sviluppo secondo la prima riga:
    $$
    1
    \begin{bmatrix}
        1 & 0\\
        0 & 2
    \end{bmatrix}
    - 0 + (-1)
    \begin{bmatrix}
        2 & 1\\
        1 & 0
    \end{bmatrix}
    =2-(-1)=\boldsymbol{3}
    $$
    Sviluppo secondo la seconda colonna:
    $$
    0 + 1 
    \begin{bmatrix}
        1 & -1\\
        1 & 2
    \end{bmatrix}
    + 0=2+1=\boldsymbol{3}
    $$
    \paragraph{Esempio} Matrice triangolare superiore ovvero $a_{ij}=0$ se i>j.
    $$
    A=
    \begin{bmatrix}
        a_{11}&*&*&*\\
        0&a_{22}&*&*\\
        0&0&\ddots&*\\
        0&0&0&a_{nn}
    \end{bmatrix}
    $$
    $$det(A)=\prod_{i=1}^{n}a_{ij}$$
    \subsection{Effetto del determinante sulle operazioni elementari di Gauss}
    \begin{enumerate}
        \item \textbf{Scambio di due righe}
            \subitem \textbf{adiacenti}: $\boldsymbol{det(A)\rightarrow -det(A)}$
            \subitem \textbf{scambio tra riga i e j qualsiasi}: si eseguono cambi successivi adiacenti.
        \item Sostituzione della riga $\boldsymbol{R_i$ con $R_i+\lambda R_j} (j \neq i)$: \textbf{det(A) non cambia}
    \end{enumerate}
    \subsection{Teoremi}
    \paragraph{Teorema} Se $A,B \in M_{nxn}(\mathbb{R})$ $$det(A \cdot B)= det(A) \cdot det(B)$$  
    \paragraph{Corollario} Se $A \in M_{nxn}(\mathbb{R})$ $$A^{-1} \text{ esiste} \Leftrightarrow det(A)\neq 0 \text{ allora } det(A^{-1})=\frac{1}{det(A)})$$
    \paragraph{Proposizione} Sia $\varphi: V \rightarrow V$ una mappa lineare, $B,B'$ due basi di V, A la matrice di $\varphi$ rispetto a B, $A'$ la matrice di $\varphi$ rispetto a $B'$.
    $$det(A)=det(A')$$
    Quindi $\boldsymbol{det(A)}$ \textbf{dipende solo da} $\boldsymbol{\varphi}$ e non dalla base.
    \paragraph{Corollario} Se $\varphi: V \rightarrow V$ è un'applicazione lineare, A la matrice di $\varphi$ rispetto ad una qualsiasi base
    $$Ker(\varphi)\neq 0 \Leftrightarrow Im(\varphi)\neq V \Leftrightarrow det(A)=0$$
    \subsection{Formula di Cramer}
    \paragraph{Definizione} Se $A \in M_{nxn}(\mathbb{R})$, la matrice \textbf{aggiunta} di A è $$\boldsymbol{\tilde{A}=[\tilde{a}_{ij}]} \text{ dove }
    \boldsymbol{\tilde{a}_{ij}=(-1)^{i+j} \; det(A_{ji})}$$ 
    \paragraph{Formula di Cramer}$$\boldsymbol{A \cdot \tilde{A}=det(A) \cdot I}$$
    dove $I \in M_{nxn}\mathbb{R}$ è la matrice identità.
    \paragraph{Corollario} Se $det(A)\neq 0$,$$\boldsymbol{A^{-1}= \frac{1}{det(A)}\cdot \tilde{A}}$$
    \paragraph{Esempio}Per n=2
    $$
    A=
    \begin{bmatrix}
        a & b\\
        c & d    
    \end{bmatrix}
    \rightarrow
    \tilde{A}=
    \begin{bmatrix}
        d & -b\\
        -c & a
    \end{bmatrix}
    $$
    \paragraph{Esempio} Per n=3
    $$A=
    \begin{bmatrix}
        1 & 0 & 3\\
        0 & 2 & 0\\
        4 & 0 & 1
    \end{bmatrix}
    \; \; \; det(A)= 2 
    \begin{bmatrix}
        1 & 3\\
        4 & 1
    \end{bmatrix}
    =-22
    $$
    $det(A)\neq 0 \Rightarrow \exists \; A^{-1}$
    $$
    \tilde{A}=
    \begin{bmatrix}
        2 & 0 & -6\\
        0 & -11 & 0\\
        -8 & 0 & 2
    \end{bmatrix}
    $$
    $$
    A^{-1}=-\frac{1}{22} \cdot \tilde{A}=
    \begin{bmatrix}
        -\frac{1}{11} & 0 & \frac{3}{11}\\
        0 & \frac{1}{2} & 0\\
        \frac{4}{11} & 0 & -\frac{1}{11}
    \end{bmatrix}
    $$
    \chapter{Autovalori ed Autovettori}
        \paragraph{Definizione} Sia V uno spazio vettoriale su $\mathbb{R}$, $dim \; V < \infty,\; \varphi: V \rightarrow V$ applicazione lineare.\\
        $\lambda \in \mathbb{R}$ è un \textbf{autovalore} di $\varphi$ se $\exists \; v \neq 0 \in V: \boldsymbol{\varphi(v)=\lambda \cdot v}$.\\
        In questo caso v è un \textbf{autovettore} associato a $\lambda$.
        \paragraph{Osservazioni}
        \begin{itemize}
            \item \textbf{v può essere autovettore per un solo} $\boldsymbol{\lambda}$
            \item $\boldsymbol{\lambda}$ \textbf{può essere autovalore di molti vettori} 
        \end{itemize}
        \paragraph{Proposizione}
        \begin{itemize}
            \item Se $\lambda \in \mathbb{R}$, gli autovettori per $\lambda$ formano un sottospazio di V:$$\boldsymbol{V_{\lambda}} \text{ \textbf{Autospazio}}$$
            \item Se $\lambda_1 \neq \lambda_2, \; V_{\lambda_1} \cap V_{\lambda_2}=(0)$
        \end{itemize}
        \clearpage
        \section{Autovalore}
            \paragraph{}$\boldsymbol{\lambda}$ \textbf{autovalore} di $\varphi \Leftrightarrow Ker(\varphi-\lambda\cdot id)\neq 0$
            \\Ma se A è la matrice di $\varphi$ rispetto ad una base $\Rightarrow
            A-\lambda I$ è la matrice di $\varphi-\lambda \cdot id$.\\
            $Ker(\varphi-\lambda \cdot id)\neq 0 \Leftrightarrow Ker(A-\lambda I)\neq 0 \Leftrightarrow$ $$\boldsymbol{det(A-\lambda I)=0}$$
            \paragraph{Definizione}Se $A \in M_{nxn} (\mathbb{R})$, il \textbf{polinomio caratteristico} di A è:
            $$\boldsymbol{P_A(t):= det(A-t\cdot I) \in \mathbb{R}[t]}$$
            dove t è una variabile.
            \paragraph{Conclusione}$\lambda$ è \textbf{autovalore} di $\boldsymbol{\varphi} \Leftrightarrow \boldsymbol{\lambda}$ \textbf{radice} di $\boldsymbol{P_A(t)}$\\
            dove A è matrice di $\varphi$ rispetto ad una base.
            $$
            A=
            \begin{bmatrix}
                a_{11}& \dots&a_{1n}\\
                a_{21}& \dots&a_{2n}\\
                \vdots& \vdots & \vdots\\
                a_{n1}&\dots&a_{nn}
            \end{bmatrix}
            \Rightarrow
            \begin{bmatrix}
                \textcolor{blue}{a_{11}-t} & a_{12} & \dots & a_{1n}\\
                a_{21} & \textcolor{blue}{a_{22}-t} & \dots & a_{2n}\\
                \vdots & \vdots & \textcolor{blue}{\ddots} & \vdots\\
                a_{n1} & a_{22}-t & \dots & \textcolor{blue}{a_{nn}-t}\\
            \end{bmatrix}
            $$
        \clearpage
        \section{Diagonalizzazione}
            \paragraph{Definizione}$\varphi$ è \textbf{diagonalizzabile} se $\exists$ una base di V dove la matrice di $\varphi$ è diagonale.
            \paragraph{Proposizione}$\varphi$ è \textbf{diagonalizzabile} $\Leftrightarrow \exists$ base di V costituita di \textbf{autovettori} di $\varphi$.
            \paragraph{Proposizione}Se $\lambda_1,\dots,\lambda_n$ sono \textbf{autovalori distinti} di $\varphi$, $v_i$
            \textbf{autovettore} associato a $\lambda_i$ dove $i=1,\dots,n \Rightarrow v_1,\dots,v_n$ sono \textbf{linearmente indipendenti}
            \paragraph{Corollario}
            \begin{enumerate}
                \item Ci sono un \textbf{numero finito} di \textbf{autovalori} distinti perché $dim \; V < \infty$.
                \item $\boldsymbol{\varphi}$ é \textbf{diagonalizzabile} $\Leftrightarrow$ se $\lambda_1,\dots,\lambda_n$ sono gli autovalori distinti di $\varphi$ allora
                $$dim \; V_{\lambda_1} + dim \; V_{\lambda_2}+\dots+dim \; V_{\lambda_n}= dim \; V$$
                \item Se $\boldsymbol{n=dim \; V$ e $\varphi}$ ha \textbf{n autovalori distinti} $\Rightarrow \boldsymbol{\varphi}$ è \textbf{diagonalizzabile}
            \end{enumerate}
            \subsection{Definire se $\boldsymbol{\varphi}$ è diagonalizzabile}
            \begin{enumerate}
                \item Trovare gli \textbf{autovalori}
                    \subitem Se ci sono n=dim V \textbf{distinti} $\Rightarrow$ \textbf{diagonalizzabile} 
                \item Trovare gli \textbf{autovettori} per ogni $\boldsymbol{\lambda_i}$ e calcolare $\boldsymbol{dim \; V_{\lambda_i}}$.
                    \subitem Se $\sum_i \; dim \; V_{\lambda_i}= dim \; V \Rightarrow$ è \textbf{diagonalizzabile}
                    \subitem Se $\sum_i \; dim \; V_{\lambda_i}< dim \; V \Rightarrow$ \textbf{NON} è \textbf{diagonalizzabile}
            \end{enumerate}
                    \subsubsection{Molteplicità}
                    \paragraph{Definizione}Sia $\lambda$ un autovalore di $\varphi$.\\
                    La \textbf{molteplicità algebrica} di $\lambda$ è la molteplicità di $\lambda$ come radice di $P_A(t)$.
                    La \textbf{molteplicità geometrica} di $\lambda$ è $dim \; V_\lambda$.
                    \paragraph{Proposizione} \textbf{Molteplicità geometrica di} $\boldsymbol{\lambda \leq}$ \textbf{molteplicità algebrica di} $\boldsymbol{\lambda}$.
                    \paragraph{Conclusione} $\boldsymbol{\varphi}$ \textbf{è diagonalizzabile} $\boldsymbol{\Leftrightarrow \forall}$ \textbf{autovalore} $\boldsymbol{\lambda}$ di $\boldsymbol{\varphi}$ è \textbf{reale} e \textbf{molteplicità algebrica = molteplicità geometrica}.\\[10px]
                    In particolare se troviamo $\boldsymbol{\lambda}$ con \textbf{molteplicità geometrica} $\boldsymbol{<}$ \textbf{molteplicità algebrica} $\Rightarrow \varphi$ \textbf{non} è \textbf{diagonalizzabile}.
                    \paragraph{Esempio} Determinare se la matrice $A=
                    \begin{bmatrix}
                        0 & 3 & 0\\
                        1 & -2 & 0\\
                        1 & -3 & 1
                    \end{bmatrix}$ è diagonalizzabile.\\
                    $P_A(t)= det 
                    \begin{bmatrix}
                        -t & 3 & 0\\
                        1 & -2t & 0\\
                        1 & -3 & 1-t
                    \end{bmatrix}
                    =(1-t)\begin{bmatrix}
                        -t & 3\\
                        1 & -2-t
                    \end{bmatrix}
                    =$\\$(1-t)(t^2+2t-3)=(1-t)(t-1)(t+3)=(t-1)^2(t+3)
                    $\\
                    \textbf{Autovalori}: 1,-3,1\\
                    \textbf{Autovettori} per \textbf{1}:
                    $\begin{bmatrix}
                        0 & 3 & 0\\
                        1 & -2 & 0\\
                        1 & -3 & 1
                    \end{bmatrix}
                    \begin{bmatrix}
                        x\\y\\z
                    \end{bmatrix}
                    =\begin{bmatrix}
                        x\\y\\z
                    \end{bmatrix}
                    \Rightarrow
                    \begin{cases}
                        3y=x\\
                        x-2y=y\\
                        x-3y+z=z
                    \end{cases}
                    $\\
                    $x=3y \Rightarrow dim(V_1)=2$\\
                    Una base di $V_1: 
                    \begin{bmatrix}
                        3\\
                        1\\
                        0
                    \end{bmatrix},
                    \begin{bmatrix}
                        3\\
                        1\\
                        1
                    \end{bmatrix}$\\
                    \textbf{Autovettori} per \textbf{-3}:
                    $\begin{bmatrix}
                        0 & 3 & 0\\
                        1 & -2 & 0\\
                        1 & -3 & 1
                    \end{bmatrix}
                    \begin{bmatrix}
                        x\\y\\z
                    \end{bmatrix}
                    =\begin{bmatrix}
                        -3x\\-3y\\-3z
                    \end{bmatrix}
                    \Rightarrow
                    \begin{cases}
                        3y=-3x\\
                        x-2y=-3y\\
                        x-3y+z=-3z
                    \end{cases}
                    $\\
                    $x=-y,-4y=-4z \Rightarrow x=-y=-z \Rightarrow dim(V_{-3})=1$\\
                    Una base di $V_{-3}: 
                    \begin{bmatrix}
                        -1\\
                        1\\
                        1
                    \end{bmatrix}$\\
                    \textbf{A è diagonalizzabile} nella base $
                    \begin{bmatrix}
                        3\\
                        1\\
                        0
                    \end{bmatrix},
                    \begin{bmatrix}
                        3\\
                        1\\
                        1
                    \end{bmatrix}
                    \begin{bmatrix}
                        -1\\
                        1\\
                        1
                    \end{bmatrix}$ dove diventa $
                    \begin{bmatrix}
                        1 & 0 & 0\\
                        0 & 1 & 0\\
                        0 & 0 & -3
                    \end{bmatrix}
                    $
                    \clearpage
            \section{Numeri complessi}
                \paragraph{}Un \textbf{numero complesso} è un'espressione $z=a+bi$ dove $a,b \in \mathbb{R}$ ed \textbf{i} è un parametro formale con la proprietà $\boldsymbol{i^2=1}$.\\
                Ogni $a \in \mathbb{R}$ è anche un numero complesso: $a=a+0i$.
                \paragraph{Notazione} $\mathbb{C}$ è l'insieme dei \textbf{numeri complessi}.
                \paragraph{Osservazione} Se $\Delta \in \mathbb{R}, \Delta < 0, (\sqrt{-\Delta}\;i)^2=\Delta \Rightarrow$\\
                $\boldsymbol{\forall}$ polinomio $\boldsymbol{ax^2+bx+c \in \mathbb{R}[x]}$ ha \textbf{radici} in $\boldsymbol{\mathbb{C}}$.
                \paragraph{Teorema}Ogni polinomio $f \in \mathbb{C}[x]$ a coefficienti \textbf{complessi} ammette una radice in $\mathbb{C}$.
                \paragraph{Definizione} Se $z=a+bi \in \mathbb{C}$, il suo \textbf{coniugato} è $\boldsymbol{\overline{z}=a-bi}$.
                \paragraph{Teorema}Se $f \in \mathbb{R}, z \in \mathbb{C}$ tale che $f(z)=0 \Rightarrow$ anche $\boldsymbol{f(\overline{z})=0}$.\\
                Quindi le \textbf{radici} di $\boldsymbol{f \in \mathbb{R}[x]}$ sono \textbf{reali} o \textbf{coppie di numeri complessi coniugati}.
                \subsection{Operazioni}
                \begin{itemize}
                    \item Se $z=a+bi, \; z'=a'+b'i$
                    \item Se $\lambda \in \mathbb{R}$
                \end{itemize}
                \paragraph{Somma}         
                $$\boldsymbol{z+z'=(a+a')+(b+b')i}$$                
                \paragraph{Prodotto con scalare}
                $$\boldsymbol{\lambda z=\lambda(a+bi)=\lambda a+\lambda bi}$$
                \paragraph{Prodotto di numeri complessi}
                $$\boldsymbol{z \cdot z'=(a+bi)(a'+b'i)=(aa'-bb')+(ab'+a'b)i}$$
        \chapter{Prodotto scalare}
        \paragraph{}Siano $
        v=
        \begin{bmatrix}
            a_1\\
            \vdots\\
            a_n
        \end{bmatrix},
        w=
        \begin{bmatrix}
            b_1\\
            \vdots\\
            b_n
        \end{bmatrix}
        $ due vettori in $\mathbb{R}^2$.\\
        Il loro \textbf{prodotto scalare} è $\boldsymbol{\langle v,w\rangle:=a_1b_1+\dots+a_nb_n \in \mathbb{R}}$.\\
        $\langle , \rangle: \mathbb{R}^n \cdot \mathbb{R}^n \rightarrow \mathbb{R}$ si chiama \textbf{prodotto scalare standard}.
        \subsection{Proprietà}
        \begin{enumerate}
            \item $\langle v,w\rangle=\langle w,v\rangle \; \forall \; v,w \in \mathbb{R}^n$
            \item Per $v \in \mathbb{R}$ \textbf{fisso}, la funzione $w\mapsto \langle v,w\rangle \in \mathbb{R}$ è un'\textbf{applicazione lineare} $\mathbb{R} \rightarrow \mathbb{R}^n$.\\
            Per $w \in \mathbb{R}$ \textbf{fisso}, la funzione $v\mapsto \langle v,w\rangle \in \mathbb{R}$ è un'\textbf{applicazione lineare} $\mathbb{R} \rightarrow \mathbb{R}^n$.
            \item $\forall \; v \in \mathbb{R}^n \; \boldsymbol{\langle v,v \rangle \geq 0}$ e $\boldsymbol{\langle v,v \rangle =0 \Leftrightarrow v=0}$
        \end{enumerate}
        \paragraph{Definizione} Se $v \in \mathbb{R}^n$, la sua \textbf{norma} è $\boldsymbol{||v||:=\sqrt{\langle v,v\rangle}}$
    \chapter{Ortogonalità e ortonormalità}
        \paragraph{Definizione} $v,w \in \mathbb{R}^n$ sono \textbf{ortogonali} se $\boldsymbol{\langle v,w \rangle=0}$\\
        Un \textbf{sistema} $v_1,\dots,v_n$ è un \textbf{sistema ortogonale} se $\boldsymbol{\langle v_i,v_j \rangle = 0 \; \forall i\neq j}$. 
        Un \textbf{sistema} $v_1,\dots,v_n$ è un \textbf{sistema ortonormale} se è \textbf{ortonormale e} $\boldsymbol{\langle v_i,v_i \rangle =1 \; \forall i}$
        \paragraph{Proposizione} Un \textbf{sistema ortogonale} di vettori è \textbf{linearmente indipendente}.
        \paragraph{Osservazione} Sia $v \in \mathbb{R}^n$. Consideriamo $\varphi_v:\mathbb{R}^n \rightarrow \mathbb{R}$. Allora:\\
        $Ker(\varphi_v)=\{w \in \mathbb{R}^n: \langle v,w \rangle =0 \}$\\
        \textbf{Notazione}: $Ker(\varphi_v)=:\langle v \rangle^\perp$.\\
        \textbf{Terminologia}: $\langle v \rangle^\perp$ è il \textbf{sottospazio ortogonale} a v.
        \subsection{Trovare sottospazio ortogonale}
        \paragraph{Generalizzazione} Se $v_1=
        \begin{bmatrix}
            a_{11}\\
            \vdots\\
            a_{1n}
        \end{bmatrix},\dots,
        \begin{bmatrix}
            a_{r1}\\
            \vdots\\
            a_{rn}
        \end{bmatrix}$ allora:
        $$\boldsymbol{\langle v_1,\dots,v_r \rangle^\perp = \text{\textbf{soluzioni del sistema omogeneo}:}}$$ 
        $$\boldsymbol{\begin{bmatrix}
            a_{11}x_1+a_{12}x_2+\dots+a_{1n}x_n=0\\
            \vdots\\
            a_{r1}x_1+a_{r2}x_2+\dots+a_{rn}x_n=0\\
        \end{bmatrix}}$$
        \paragraph{Esempio} Siano $v=
        \begin{bmatrix}
            1\\
            1\\
            1\\
            0
        \end{bmatrix},
        \; w=
        \begin{bmatrix}
            0\\
            1\\
            1\\
            1
        \end{bmatrix}
        \in \mathbb{R}^4$. Troviamo il sottospazio $\langle v,w \rangle^\perp$.\\
        $\langle v,w \rangle^\perp$ è il \textbf{sottospazio delle soluzioni}:
        $
        \begin{cases}
            x_1+x_2+x_3=0\\
            x_2+x_3+x_4=0
        \end{cases}
        $\\
        Troviamo una base:
        $
        \begin{bmatrix}
            1 & 1 & 1 & 0\\
            0 & 1 & 1 & 1
        \end{bmatrix}
        \rightarrow
        \begin{bmatrix}
            1 & 0 & 0 & -1\\
            0 & 1 & 1 & 1
        \end{bmatrix}
        \begin{cases}
            x_1=x_4\\
            x_2=-x_3-x_4
        \end{cases}
        $\\
        \textbf{Soluzione generale:}
        $$
        \begin{bmatrix}
            s\\
            -s-t\\
            t\\
            s
        \end{bmatrix}=s
        \begin{bmatrix}
            1\\
            -1\\
            0\\
            1
        \end{bmatrix}+t
        \begin{bmatrix}
            0\\
            -1\\
            1\\
            0
        \end{bmatrix}
        \Rightarrow
        \boldsymbol{v_1, v_2} \text{ sono una \textbf{base} di } \boldsymbol{\langle v,w \rangle^\perp}. 
        $$
        \subsection{Base ortonormale}
        \paragraph{Teorema} Ogni sottospazio $V \subset \mathbb{R}^n$ ammette una base \textbf{ortonormale}.
        \paragraph{Corollario} Sia $\boldsymbol{v_1,\dots,v_r}$ un \textbf{sistema ortonormale} in $\mathbb{R}^n$. Allora\\
        $\exists \; v_{r+1},\dots,v_n \in \mathbb{R}^n: v_1,\dots,v_r,v_r+1,\dots,v_n$ è una \textbf{base ortonormale (ON)} di $\mathbb{R}^n$.
        \paragraph{Corollario} $dim \; \langle v_1,\dots,v_r \rangle^\perp = n-r$.
        \paragraph{Corollario} \mbox{}\\
        Sia $V \subset \mathbb{R}^n$ un sottospazio, sia $V^\perp:=\{w \in \mathbb{R}^n: \langle v,w \rangle=0, \forall v \in V\}$
        allora $V \cap V^\perp=\{0\}$ e $Span(V,V^\perp)=\mathbb{R}^n$ e in :particolare $$\boldsymbol{dim \; V+ dim \; V^\perp=n}$$
        \subsubsection{A cosa serve una base ortonormale?}
        Se $v_1,\dots,v_n$ è una base \textbf{ortonormale} di $\mathbb{R}^n$ allora $$\boldsymbol{v=\sum^n_{i=1} \langle v,v_i \rangle v_i} \; \; \forall v \in \mathbb{R}^n$$
        \subsubsection{Trovare una base ortonormale}
        \begin{itemize}
            \item $dim \; V=1$ Se $V=\langle v \rangle, \overline{v_1}=\frac{v}{||v||}$ è una \textbf{base ortonormale}.
            \item $dim \; V=1$ Se $v_1,v_2$ base di $V, \overline{v_1}$
        \end{itemize}
        \subsubsection{Algoritmo di Gram-Schmidt}
        \paragraph{}Se $v_1,\dots,v_n$ sono vettori \textbf{linearmente indipendenti} di V costruiamo un insieme di \textbf{vettori ortogonali} $\boldsymbol{w_1,...
        ,w_n}$.
        \begin{itemize}
            \item $w_1=v_1$
            \item $w_2=v_2-\frac{\langle v_2,w_1\rangle}{\langle w_1,w_1 \rangle}w_1$
            \item $w_3=v_3-\frac{\langle v_3,w_1\rangle}{\langle w_1,w_1 \rangle}w_1-\frac{\langle v_3,w_2\rangle}{\langle w_2,w_2 \rangle}w_2$
            \item \dots
            \item $w_n=v_n-\frac{\langle v_n,w_1\rangle}{\langle w_1,w_1\rangle}w_1-\frac{\langle v_n,w_2\rangle}{\langle w_2,w_2\rangle}w_2-\dots-\frac{\langle v_{n},w_{n-1}\rangle}{\langle w_{n-1},w_{n-1}\rangle}w_{n-1}$
        \end{itemize}
        Trovati $\boldsymbol{w_1,\dots,w_n}$ vettori \textbf{ortogonali} li \textbf{normalizziamo}:
        $$\boldsymbol{w'_i=\frac{w_i}{||w_i||}}$$
        I vettori $\boldsymbol{w'_1,\dots,w'_n}$ formeranno una \textbf{base ortonormale}.
    \chapter{Teorema spettrale}
        \paragraph{Definizione} $A \in \mathbb{M}_{nxn}$ è \textbf{simmetrica} se $\boldsymbol{a_{ij}=a_{ji} \; \forall i,j}$ oppure se $A=A^T$ dove $\boldsymbol{A^T}$ è la \textbf{matrice trasposta}.
        \paragraph{Lemma} Se $A \in M_{nxn}(\mathbb{R})$ è simmetrica, allora $$\boldsymbol{\langle Av,w \rangle = \langle v,Aw \rangle} \;\; \forall v,w \in \mathbb{R}^n$$
        \paragraph{Teorema spettrale per matrici simmetriche}\mbox{}\\
        Se $A \in M_{nxn}(\mathbb{R})$ è \textbf{simmetrica}, ogni \textbf{autovalore} di A è \textbf{reale} ed A è \textbf{diagonalizzabile} in una base \textbf{ortonormale} di \textbf{autovalori}. 
\end{document}